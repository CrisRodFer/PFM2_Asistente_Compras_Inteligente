{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f514c496",
   "metadata": {},
   "source": [
    "# **PFM2 ‚Äì Modelado y Aplicaci√≥n Pr√°ctica.**\n",
    "\n",
    "### Punto de partida\n",
    "Este notebook contin√∫a directamente el trabajo realizado en las Fases 1‚Äì6, utilizando como insumo el dataset validado `data/processed/subset_modelado.parquet`.  \n",
    "Dicho dataset incluye:  \n",
    "- La demanda original y ajustada.  \n",
    "- La etiqueta `is_outlier` (procedente de DBSCAN).  \n",
    "- Las nuevas columnas de trazabilidad anual (`tipo_outlier_year` y `decision_outlier_year`) generadas en la Fase 6.  \n",
    "\n",
    "Este punto de partida garantiza que el modelado se apoya sobre datos consistentes, libres de anomal√≠as espurias y con informaci√≥n de contexto suficiente para interpretar se√±ales de negocio.\n",
    "\n",
    "### Objetivo\n",
    "Entrenar y evaluar modelos de predicci√≥n de demanda robustos, comparando diferentes enfoques (modelos estad√≠sticos, machine learning y enfoques h√≠bridos) y evaluando su capacidad para:  \n",
    "- Integrar se√±ales clave como top ventas y eventos de calendario.  \n",
    "- Capturar tendencias, estacionalidades y picos de forma coherente.  \n",
    "- Servir como base para la construcci√≥n de una aplicaci√≥n interactiva en **Streamlit**, que permita al usuario explorar, simular y consumir las previsiones en un entorno operativo.  \n",
    "\n",
    "### Referencia metodol√≥gica\n",
    "Para una descripci√≥n detallada del tratamiento de outliers y validaciones aplicadas, ver `reports/outliers/outliers_resumen.csv` y el notebook de Fase 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e969309",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbc8b99",
   "metadata": {},
   "source": [
    "### **√çndice de Contenidos**\n",
    "\n",
    "#### Fase 1: Inspecci√≥n y preparaci√≥n de datasets de partida\n",
    "- 1.1. Reetiquetado temporal de la previsi√≥n de demanda (2025).\n",
    "- 1.2. Revisi√≥n y reetiquetado del hist√≥rico 2023.\n",
    "- 1.3. Revisi√≥n y reetiquetado del hist√≥rico 2022.\n",
    "- 1.4. Limpieza y preparaci√≥n del cat√°logo de productos.\n",
    "- 1.5. Coherencia entre los datos hist√≥ricos.\n",
    "  - 1.5.1. An√°lisis visual.\n",
    "  - 1.5.2. An√°lisis estad√≠stico.\n",
    "  - 1.5.3. Contraste de hip√≥tesis.\n",
    "  - 1.5.4. Conclusiones finales.\n",
    "\n",
    "#### Fase 2: Desagregaci√≥n de los datos\n",
    "- 2.1. Generaci√≥n del patr√≥n estacional para la desagregaci√≥n de la demanda.\n",
    "  - 2.1.1. Aplicaci√≥n del patr√≥n estacional por a√±o (2022‚Äì2024).\n",
    "  - 2.1.2. Validaci√≥n del calendario estacional.\n",
    "- 2.2. Aplicaci√≥n del patr√≥n estacional a la demanda anual.\n",
    "  - 2.2.1. Desagregaci√≥n diaria del a√±o 2024.\n",
    "  - 2.2.2. Desagregaci√≥n diaria del a√±o 2023.\n",
    "  - 2.2.3. Desagregaci√≥n diaria del a√±o 2022.\n",
    "  - 2.2.4. Conclusiones de la desagregaci√≥n de demanda diaria.\n",
    "- 2.3. Comparativa entre a√±os: ¬øse ha aplicado bien la estacionalidad?\n",
    "  - 2.3.1. Evoluci√≥n diaria total por a√±o (curva cruda + suavizada).\n",
    "  - 2.3.2.  Correlaci√≥n de las curvas diarias agregadas (2022‚Äì2024).\n",
    "  - 2.3.3. Demanda media diaria mensual por a√±o.\n",
    "  - 2.3.4. KPIs de consistencia (CV mensual y correlaciones).\n",
    "  - 2.3.5. Validaci√≥n extra con calendario real (Espa√±a 2022‚Äì2024).\n",
    "  - 2.3.6. Conclusiones de la validaci√≥n estacional y configuraci√≥n definitiva.\n",
    "\n",
    "#### Fase 3: Construcci√≥n del subset representativo.\n",
    "- 3.1. Unificaci√≥n de demandas (2022‚Äì2024).\n",
    "- 3.2. Cruce con cat√°logo y asociaci√≥n de categor√≠as.\n",
    "- 3.3. Filtrado de casos problem√°ticos.\n",
    "- 3.4. Reducci√≥n de dimensionalidad (PCA sobre categor√≠as).\n",
    "- 3.5. Clustering de productos.\n",
    "- 3.6. Generaci√≥n del subset representativo.\n",
    "\n",
    "#### Fase 4: Impacto del precio sobre la demanda.\n",
    "- 4.1. Objetivo, datos de partida y mapeo de columnas y dise√±os del efecto precio (ventanas + elasticidades).\n",
    "- 4.2. Preflight de ventanas ‚Äî `ventanas_precio.py`\n",
    "- 4.3. Aplicaci√≥n del efecto ‚Äî `aplicar_efecto_precio.py`\n",
    "- 4.4. Validaci√≥n r√°pida (sanity).\n",
    "- 4.5. Validaci√≥n adicional: alineamiento con calendario real.\n",
    "\n",
    "#### Fase 5: Aplicaci√≥n de factores externos y simulaci√≥n de escenarios.\n",
    "- 5.1. Introducci√≥n y objetivos.\n",
    "- 5.2. Definici√≥n de factores externos.\n",
    "- 5.3. Dise√±o del modelo de aplicaci√≥n.\n",
    "- 5.4. Implementaci√≥n en c√≥digo.\n",
    "- 5.5. Validaci√≥n de coherencia y robustez.\n",
    "  - 5.5.1. Validaci√≥n de coherencia del precio.\n",
    "  - 5.5.2. Validaci√≥n adicional (alineamiento ventanas).\n",
    "  - 5.5.3. Comparativa de demanda.\n",
    "  - 5.5.4. Validaci√≥n de trazabilidad.\n",
    "- 5.6. Conclusiones de la fase 5.\n",
    "\n",
    "#### Fase 6: An√°lisis y tratamiento de outliers.\n",
    "- 6.1. Validaci√≥n complementaria: b√∫squeda de nuevos candidatos.\n",
    "- 6.2. An√°lisis de outliers detectados por DBSCAN.\n",
    "- 6.3. Resultados consolidados y decisiones finales.\n",
    "- 6.4. Implicaciones para el modelado.\n",
    "  - 6.4.1. Integraci√≥n en el subset final.\n",
    "  - 6.4.2. Visualizaci√≥n del impacto de outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e21050",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4521e8",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Nota metodol√≥gica sobre los datos hist√≥ricos utilizados.**\n",
    "\n",
    "Los datos hist√≥ricos correspondientes a los ejercicios 2022‚Äì2024 no proceden de registros reales de ventas, sino que fueron **generados a partir de la previsi√≥n de demanda 2025**. \n",
    "Para construir estos hist√≥ricos se aplicaron de manera controlada diversos componentes que reflejan el comportamiento esperado en un contexto de comercio electr√≥nico:\n",
    "\n",
    "- **Patr√≥n estacional**: incorporaci√≥n de estacionalidad diaria y anual (ciclos de ingresos mensuales, rebajas, campa√±as como Black Friday, Prime Day, etc.).\n",
    "\n",
    "- **Impacto del precio**: simulaci√≥n del efecto del precio sobre la demanda, con distinta sensibilidad por cl√∫ster de producto.\n",
    "\n",
    "- **Factores externos**: inclusi√≥n de variables de calendario y eventos promocionales como dummies ex√≥genas.\n",
    "\n",
    "- **Ruido controlado y aleatorio**: a√±adido de perturbaciones aleatorias con distribuci√≥n normal, calibradas para introducir variabilidad sin distorsionar las tendencias de fondo.\n",
    "\n",
    "> Este enfoque busc√≥ **evitar la circularidad** inherente a la construcci√≥n de hist√≥ricos a partir de una previsi√≥n futura, de manera que los modelos no aprendan relaciones deterministas y conserven capacidad de generalizaci√≥n.\n",
    "\n",
    "üõë **Limitaciones**\n",
    "\n",
    "No obstante, este planteamiento presenta ciertas limitaciones que deben ser tenidas en cuenta en la interpretaci√≥n de los resultados:\n",
    "\n",
    "- Los datos de 2022‚Äì2024 heredan en gran medida las tendencias y estacionalidades de la previsi√≥n 2025, lo que puede reducir la \n",
    "  diversidad de patrones respecto a hist√≥ricos reales.\n",
    "\n",
    "- El ruido introducido, aunque aleatorio, no refleja en su totalidad la complejidad de desviaciones reales  \n",
    "  (errores humanos, incidencias log√≠sticas, cambios imprevistos de mercado).\n",
    "\n",
    "- La validaci√≥n mediante backtesting sobre 2024 se realiza frente a un hist√≥rico simulado a partir de 2025, lo que podr√≠a generar resultados \n",
    "  algo m√°s optimistas que en un entorno con datos 100% reales.\n",
    "\n",
    "üîç **Enfoque adoptado**\n",
    "\n",
    "A pesar de estas limitaciones, el enfoque es **v√°lido y adecuado** para los objetivos del proyecto porque:\n",
    "\n",
    "- Permite **evaluar de manera realista la metodolog√≠a de predicci√≥n y el pipeline completo**(desde la generaci√≥n de features hasta la selecci√≥n de modelos).\n",
    "\n",
    "- Introduce suficiente variabilidad y ruido para que los algoritmos deban **aprender patrones** y no simplemente replicar la previsi√≥n original.\n",
    "\n",
    "- Facilita la comparaci√≥n objetiva entre diferentes familias de modelos y la selecci√≥n por cl√∫ster en base a m√©tricas robustas (sMAPE, WAPE, MAE ponderado).\n",
    "\n",
    "> En conclusi√≥n, los hist√≥ricos generados proporcionan un marco de prueba **coherente y consistente** para validar la l√≥gica del sistema de predicci√≥n y simulaci√≥n de stock, \n",
    "entendiendo que los resultados no equivalen a un backtesting sobre datos 100% reales, sino a un escenario controlado que reproduce condiciones veros√≠miles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b147c5",
   "metadata": {},
   "source": [
    "üìå **Nota metodol√≥gica final sobre outliers y clusters**\n",
    "\n",
    "En la Fase 2, a partir del clustering con DBSCAN, un conjunto reducido de productos qued√≥ marcado como outliers. En lugar de eliminarlos del subset (como se hizo en clase), se decidi√≥ mantenerlos en el dataset, ya que el an√°lisis posterior mostr√≥ que estos productos coincid√≠an con dos situaciones:\n",
    "\n",
    "- **Top ventas** ‚Üí productos de alta rotaci√≥n cuya exclusi√≥n hubiera distorsionado la demanda real.\n",
    "- **Picos aislados coherentes** ‚Üí ventas puntuales pero justificadas por campa√±as, estacionalidad o ventanas de grandes ventas.\n",
    "\n",
    "Durante la Fase 6, para garantizar que todos los productos participaran en el modelado por cl√∫ster, se cre√≥ la columna __cluster__.\n",
    "\n",
    "- En los productos no outliers (is_outlier = 0), cluster y __cluster__ son id√©nticos.\n",
    "- En los productos outliers (is_outlier = 1), se aplic√≥ un **criterio de fallback determinista**, asign√°ndolos al cl√∫ster mayoritario (cl√∫ster 1).\n",
    "\n",
    "**Limitaciones**\n",
    "\n",
    "- Este enfoque diluye en cierta medida la especificidad de los outliers.\n",
    "- Sin embargo, dado que en este caso **todos los outliers estaban justificados** (bien por ser top ventas, bien por picos coherentes con la √©poca), su integraci√≥n en el cl√∫ster mayoritario no compromete la validez del modelo.\n",
    "\n",
    "**Enfoque adoptado**\n",
    "\n",
    "- Se opta por mantener la asignaci√≥n al cl√∫ster mayoritario para no dejar productos fuera del pipeline.\n",
    "- Se documenta esta decisi√≥n como un compromiso entre simplicidad, cobertura y coherencia de negocio.\n",
    "- Como l√≠nea futura, se podr√≠a explorar una reasignaci√≥n basada en distancias a centroides u otras m√©tricas, pero no se considera necesaria en esta fase.\n",
    "\n",
    "\n",
    "**Posible l√≠nea futura: clustering espec√≠fico de outliers**\n",
    "\n",
    "En el presente proyecto los productos identificados como outliers fueron integrados en el cl√∫ster mayoritario con el objetivo de garantizar su cobertura en \n",
    "el modelado y evitar su eliminaci√≥n, dado que en su mayor√≠a correspond√≠an a top ventas o a picos de demanda coherentes con la estacionalidad.\n",
    "\n",
    "Como l√≠nea de trabajo futura, se podr√≠a plantear un clustering espec√≠fico sobre el conjunto de outliers. Esta estrategia permitir√≠a identificar subgrupos internos \n",
    "(por ejemplo, distinguir entre productos con alta rotaci√≥n recurrente frente a productos con picos estacionales aislados) y, en consecuencia, aplicar modelos diferenciados m√°s ajustados a cada comportamiento.\n",
    "\n",
    "No obstante, dado que el volumen de productos outliers es reducido respecto al total (alrededor de un 5‚Äì6 %) y que los modelos con variables ex√≥genas ya permiten explicar \n",
    "sus patrones de manera satisfactoria, se considera que esta extensi√≥n no es necesaria en la versi√≥n actual del modelo y se pospone como l√≠nea futura de refinamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c8ea7f",
   "metadata": {},
   "source": [
    "## FASE 7: **Validaci√≥n y preparaci√≥n del dataset para el modelado**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e984ad",
   "metadata": {},
   "source": [
    "### **7.1. Validaci√≥n inicial del dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b30e0a9",
   "metadata": {},
   "source": [
    "\n",
    "El primer paso antes de comenzar con el modelado consiste en realizar una **validaci√≥n exhaustiva del dataset de partida**.  \n",
    "El objetivo de este bloque es garantizar que los datos sobre los que se entrenar√°n los modelos son **consistentes, completos y utilizables**, evitando que errores estructurales condicionen los resultados posteriores.\n",
    "\n",
    "üéØ **Objetivo**\n",
    "- Comprobar que la **variable objetivo** (`demand_final_noised`) no presenta valores nulos ni negativos.\n",
    "- Verificar que las **fechas** cubren el rango esperado (2022‚Äì2024) y que no existen duplicados en la combinaci√≥n (`product_id`, `date`).\n",
    "- Identificar posibles problemas de cobertura temporal (fechas faltantes, series constantes, productos incompletos).\n",
    "- Validar que todos los **productos tienen un cl√∫ster asignado** y que la informaci√≥n de outliers est√° correctamente registrada.\n",
    "- Revisar de forma preliminar las **variables de precio y factores externos**.\n",
    "\n",
    "‚ùì **Por qu√© se realiza**\n",
    "Una validaci√≥n previa es esencial porque:\n",
    "- Asegura que los **modelos trabajen con datos coherentes** y sin inconsistencias.\n",
    "- Evita que los resultados del backtesting est√©n sesgados por errores de entrada.\n",
    "- Permite identificar productos o periodos problem√°ticos antes de invertir tiempo en el entrenamiento.\n",
    "\n",
    "üõ†Ô∏è **C√≥mo se lleva a cabo**\n",
    "La validaci√≥n se efect√∫a mediante un **script espec√≠fico** (`validacion_dataset_modelado.py`) que genera un reporte con:\n",
    "- Informaci√≥n general del dataset.\n",
    "- Estado de la variable objetivo.\n",
    "- Cobertura temporal por producto.\n",
    "- Comprobaciones sobre cl√∫steres y outliers.\n",
    "- Un **resumen tipo sem√°foro** (OK/NO-OK) de las validaciones cr√≠ticas.\n",
    "\n",
    "> De esta manera, cualquier problema estructural queda documentado y puede ser corregido antes de pasar a la fase de preparaci√≥n de datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04e815e",
   "metadata": {},
   "source": [
    "‚öôÔ∏è **Script: `validacion_dataset_modelado.py`**\n",
    "\n",
    "üéØ **Objetivo.**  \n",
    "Automatizar la validaci√≥n del dataset de modelado, comprobando la integridad de la variable objetivo, la cobertura temporal, los cl√∫steres y la trazabilidad de los outliers. Este script act√∫a como herramienta de diagn√≥stico previa al modelado.\n",
    "\n",
    "‚û°Ô∏è **Entradas.**\n",
    "- `data/processed/subset_modelado.parquet` (dataset validado en Fases 1‚Äì6).\n",
    "\n",
    "‚¨ÖÔ∏è **Salidas.**\n",
    "- Reporte en consola con todos los resultados de validaci√≥n.  \n",
    "- (Opcional) Archivo TXT si se especifica `--report`.\n",
    "\n",
    "üîÅ **Flujo de trabajo.**\n",
    "1. **Carga del dataset** (Parquet).  \n",
    "2. **Chequeo de columnas y tipos** (`df.info()` capturado en buffer).  \n",
    "3. **Validaci√≥n de la variable objetivo**: nulos, negativos, estad√≠sticos b√°sicos.  \n",
    "4. **Cobertura temporal**: fechas m√≠nimas/m√°ximas globales y por producto; detecci√≥n de duplicados `product_id+date`; c√°lculo de completitud diaria.  \n",
    "5. **Series constantes**: identifica productos con demanda sin variaci√≥n.  \n",
    "6. **Precio y factores**: detecci√≥n de valores nulos/negativos en columnas relevantes (`precio_medio`, `price_factor_effective`).  \n",
    "7. **Validaci√≥n de cl√∫steres**: confirmaci√≥n de que todos los productos tienen cl√∫ster asignado; coherencia `cluster` vs `__cluster__` en productos no-outlier.  \n",
    "8. **Outliers**: verificaci√≥n de columnas relacionadas, recuento de productos marcados y n√∫mero de cl√∫steres asignados.  \n",
    "9. **Resumen ‚Äúsem√°foro‚Äù**: indicadores booleanos (`OK=True/False`) de las comprobaciones cr√≠ticas.\n",
    "\n",
    "ü™õ **Par√°metros modificables.**\n",
    "- Rutas de entrada y salida (`--in`, `--report`).\n",
    "- Nombre de la variable objetivo (`demand_final_noised` por defecto).\n",
    "\n",
    "üß© **Ejecuci√≥n.**\n",
    "- CLI:  \n",
    "  ```bash\n",
    "  python scripts/eda/validacion_dataset_modelado.py\n",
    "  python scripts/eda/validacion_dataset_modelado.py --report reports/validacion_dataset.txt\n",
    "\n",
    "- Notebook:\n",
    "\n",
    " `from scripts.eda.validacion_dataset_modelado import run_validation`\n",
    " \n",
    " `print(run_validation())`\n",
    "\n",
    "üìù **Notas.**\n",
    "- El script no modifica el dataset original.\n",
    "- Si se encuentra alg√∫n problema cr√≠tico (ej. nulos en target, fechas fuera de rango), debe ser corregido antes de continuar con el modelado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86f36272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 11:29:18,994 | INFO | notebook.validacion_dataset_modelado | Leyendo: C:\\Users\\crisr\\Desktop\\M√°ster Data Science & IA\\PROYECTO\\PFM2_Asistente_Compras_Inteligente\\data\\processed\\subset_modelado.parquet\n",
      "2025-09-08 11:29:19,657 | INFO | notebook.validacion_dataset_modelado | Validando‚Ä¶\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ENCABEZADOS ===\n",
      "['precio_medio', 'product_id', 'demand_day', 'is_outlier', 'cluster', 'date', '__cluster__', '__product_id__', 'demand_multiplier', 'demand_day_priceadj', 'price_factor_effective', 'price_virtual', 'm_agosto_nonprice', 'm_competition', 'm_inflation', 'm_promo', 'm_seasonextra', 'm_segments', 'demand_final', 'factors_applied', 'demand_final_noised', 'demand_final_noiseds_adj', 'year', 'tipo_outlier_year', 'decision_outlier_year']\n",
      "\n",
      "=== INFO ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3941216 entries, 0 to 3941215\n",
      "Data columns (total 25 columns):\n",
      " #   Column                    Non-Null Count    Dtype         \n",
      "---  ------                    --------------    -----         \n",
      " 0   precio_medio              3941216 non-null  float64       \n",
      " 1   product_id                3941216 non-null  string        \n",
      " 2   demand_day                3941216 non-null  float64       \n",
      " 3   is_outlier                3941216 non-null  int64         \n",
      " 4   cluster                   3720920 non-null  float64       \n",
      " 5   date                      3941216 non-null  datetime64[ns]\n",
      " 6   __cluster__               3941216 non-null  int64         \n",
      " 7   __product_id__            3941216 non-null  int64         \n",
      " 8   demand_multiplier         3941216 non-null  float64       \n",
      " 9   demand_day_priceadj       3941216 non-null  float64       \n",
      " 10  price_factor_effective    3941216 non-null  float64       \n",
      " 11  price_virtual             3941216 non-null  float64       \n",
      " 12  m_agosto_nonprice         3941216 non-null  float64       \n",
      " 13  m_competition             3941216 non-null  float64       \n",
      " 14  m_inflation               3941216 non-null  float64       \n",
      " 15  m_promo                   3941216 non-null  float64       \n",
      " 16  m_seasonextra             3941216 non-null  float64       \n",
      " 17  m_segments                3941216 non-null  float64       \n",
      " 18  demand_final              3941216 non-null  float64       \n",
      " 19  factors_applied           3941216 non-null  object        \n",
      " 20  demand_final_noised       3941216 non-null  float64       \n",
      " 21  demand_final_noiseds_adj  3720920 non-null  float64       \n",
      " 22  year                      3941216 non-null  Int64         \n",
      " 23  tipo_outlier_year         3941216 non-null  object        \n",
      " 24  decision_outlier_year     3941216 non-null  object        \n",
      "dtypes: Int64(1), datetime64[ns](1), float64(16), int64(3), object(3), string(1)\n",
      "memory usage: 755.5+ MB\n",
      "\n",
      "=== TARGET (demand_final_noised) ===\n",
      "Nulos: 0\n",
      "Negativos: 0\n",
      "count    3.941216e+06\n",
      "mean     2.130598e+00\n",
      "std      1.001068e+00\n",
      "min      1.000000e+00\n",
      "25%      1.000000e+00\n",
      "50%      2.000000e+00\n",
      "75%      3.000000e+00\n",
      "max      1.500000e+01\n",
      "Name: demand_final_noised, dtype: float64\n",
      "\n",
      "=== COBERTURA GLOBAL DE FECHAS ===\n",
      "Min: 2022-01-01 00:00:00  |  Max: 2024-12-31 00:00:00\n",
      "\n",
      "Duplicados (product_id, date): 0\n",
      "Productos con fechas faltantes: 0\n",
      "Completitud media %: 100.0\n",
      "Productos con demanda constante (√∫nico valor): 265\n",
      "\n",
      "=== CHEQUEO precio_medio ===\n",
      "Nulos: 0 | Negativos: 0 | Min: 5.09 | Max: 99.99\n",
      "\n",
      "=== CHEQUEO price_factor_effective ===\n",
      "Nulos: 0 | Negativos: 0 | Min: 0.75 | Max: 1.0\n",
      "\n",
      "=== CL√öSTERES (__cluster__) ===\n",
      "Productos √∫nicos: 3596\n",
      "Productos con cluster: 3596\n",
      "Productos SIN cluster: 0\n",
      "Cluster y __cluster__ id√©nticos en NO-outliers: True\n",
      "\n",
      "=== COLUMNAS OUTLIERS ===\n",
      "['is_outlier', 'tipo_outlier_year', 'decision_outlier_year']\n",
      "Productos outlier: 201\n",
      "Clusters distintos en outliers: 1\n",
      "\n",
      "=== RESUMEN (OK=True) ===\n",
      "target_sin_nulos: True\n",
      "target_sin_negativos: True\n",
      "sin_duplicados_pid_fecha: True\n",
      "cluster_cubierto: True\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Script: validaci√≥n_dataset_modelado.py\n",
    "# Validaci√≥n inicial del dataset de modelado\n",
    "# Objetivo: foto r√°pida y completa de calidad de datos y trazabilidad de cl√∫ster/outliers\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import logging\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------- Helper: encontrar ra√≠z del repo (carpeta que contenga data/processed) ----------\n",
    "def find_repo_root(start: Path | None = None) -> Path:\n",
    "    p = Path(start or Path.cwd()).resolve()\n",
    "    for parent in (p, *p.parents):\n",
    "        if (parent / \"data\" / \"processed\").exists():\n",
    "            return parent\n",
    "    return p  # fallback: cwd si no encuentra nada\n",
    "\n",
    "# ---------- Rutas por defecto (funciona en script y en notebook) ----------\n",
    "if \"__file__\" in globals():\n",
    "    _start = Path(__file__).resolve().parent\n",
    "    LOGGER_NAME = Path(__file__).stem\n",
    "else:\n",
    "    _start = Path.cwd()\n",
    "    LOGGER_NAME = \"notebook.validacion_dataset_modelado\"\n",
    "\n",
    "ROOT_DIR = find_repo_root(_start)\n",
    "PROCESSED_DIR = ROOT_DIR / \"data\" / \"processed\"\n",
    "\n",
    "# ---------- Logging ----------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    ")\n",
    "log = logging.getLogger(LOGGER_NAME)\n",
    "\n",
    "\n",
    "# ---------- N√∫cleo de validaci√≥n ----------\n",
    "def validate_dataset(df: pd.DataFrame, target: str = \"demand_final_noised\") -> str:\n",
    "    \"\"\"Devuelve un string con el reporte de validaci√≥n.\"\"\"\n",
    "    lines: list[str] = []\n",
    "\n",
    "    # 1) Columnas / tipos\n",
    "    lines.append(\"=== ENCABEZADOS ===\")\n",
    "    lines.append(str(list(df.columns)))\n",
    "\n",
    "    lines.append(\"\\n=== INFO ===\")\n",
    "    buf = io.StringIO()                       # <- buffer v√°lido para df.info()\n",
    "    df.info(buf=buf, show_counts=True)\n",
    "    lines.extend(buf.getvalue().splitlines())\n",
    "\n",
    "    # 2) Target\n",
    "    assert target in df.columns, f\"No existe la columna objetivo '{target}'\"\n",
    "    tgt = df[target]\n",
    "    lines.append(f\"\\n=== TARGET ({target}) ===\")\n",
    "    lines.append(f\"Nulos: {int(tgt.isna().sum())}\")\n",
    "    lines.append(f\"Negativos: {int((tgt < 0).sum())}\")\n",
    "    lines.append(str(tgt.describe()))\n",
    "\n",
    "    # 3) Fechas y cobertura\n",
    "    assert \"date\" in df.columns, \"Falta columna 'date'\"\n",
    "    df = df.copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    lines.append(\"\\n=== COBERTURA GLOBAL DE FECHAS ===\")\n",
    "    lines.append(f\"Min: {df['date'].min()}  |  Max: {df['date'].max()}\")\n",
    "\n",
    "    # Duplicados product_id+date\n",
    "    dups = int(df.duplicated([\"product_id\", \"date\"]).sum())\n",
    "    lines.append(f\"\\nDuplicados (product_id, date): {dups}\")\n",
    "\n",
    "    # Continuidad diaria por producto\n",
    "    span = df.groupby(\"product_id\")[\"date\"].agg([\"min\", \"max\", \"count\"])\n",
    "    span[\"dias_esperados\"] = (span[\"max\"] - span[\"min\"]).dt.days + 1\n",
    "    span[\"completitud_%\"] = (span[\"count\"] / span[\"dias_esperados\"] * 100).round(2)\n",
    "    faltantes = int((span[\"completitud_%\"] < 100).sum())\n",
    "    lines.append(f\"Productos con fechas faltantes: {faltantes}\")\n",
    "    lines.append(f\"Completitud media %: {span['completitud_%'].mean().round(2)}\")\n",
    "\n",
    "    # Series constantes\n",
    "    var0 = int((df.groupby(\"product_id\")[target].nunique() == 1).sum())\n",
    "    lines.append(f\"Productos con demanda constante (√∫nico valor): {var0}\")\n",
    "\n",
    "    # 4) Precio (si existe)\n",
    "    for col in [\"precio_medio\", \"price_factor_effective\"]:\n",
    "        if col in df.columns:\n",
    "            lines.append(f\"\\n=== CHEQUEO {col} ===\")\n",
    "            lines.append(\n",
    "                f\"Nulos: {int(df[col].isna().sum())} | Negativos: {int((df[col] < 0).sum())} \"\n",
    "                f\"| Min: {df[col].min()} | Max: {df[col].max()}\"\n",
    "            )\n",
    "\n",
    "    # 5) Cl√∫steres\n",
    "    cluster_col = \"__cluster__\" if \"__cluster__\" in df.columns else (\"cluster\" if \"cluster\" in df.columns else None)\n",
    "    assert cluster_col is not None, \"No hay columna de cluster ni __cluster__\"\n",
    "    lines.append(f\"\\n=== CL√öSTERES ({cluster_col}) ===\")\n",
    "    lines.append(f\"Productos √∫nicos: {df['product_id'].nunique()}\")\n",
    "    lines.append(f\"Productos con cluster: {df.loc[df[cluster_col].notna(), 'product_id'].nunique()}\")\n",
    "    lines.append(f\"Productos SIN cluster: {df.loc[df[cluster_col].isna(), 'product_id'].nunique()}\")\n",
    "\n",
    "    # Coherencia en NO-outliers\n",
    "    if {\"cluster\", \"__cluster__\", \"is_outlier\"}.issubset(df.columns):\n",
    "        no_out = df[\"is_outlier\"].eq(0)\n",
    "        iguales = (df.loc[no_out, \"cluster\"].fillna(-1).astype(int)\n",
    "                   == df.loc[no_out, \"__cluster__\"].astype(int)).all()\n",
    "        lines.append(f\"Cluster y __cluster__ id√©nticos en NO-outliers: {bool(iguales)}\")\n",
    "\n",
    "    # 6) Outliers\n",
    "    outlier_cols = [c for c in df.columns if \"outlier\" in c.lower()]\n",
    "    lines.append(\"\\n=== COLUMNAS OUTLIERS ===\")\n",
    "    lines.append(str(outlier_cols))\n",
    "    if \"is_outlier\" in df.columns:\n",
    "        n_out = int(df.query(\"is_outlier == 1\")[\"product_id\"].nunique())\n",
    "        lines.append(f\"Productos outlier: {n_out}\")\n",
    "        asign = df.loc[df[\"is_outlier\"] == 1, [\"product_id\", cluster_col]].drop_duplicates()\n",
    "        lines.append(f\"Clusters distintos en outliers: {asign[cluster_col].nunique()}\")\n",
    "\n",
    "    # 7) Resumen sem√°foro\n",
    "    checks = {\n",
    "        \"target_sin_nulos\": int(tgt.isna().sum()) == 0,\n",
    "        \"target_sin_negativos\": int((tgt < 0).sum()) == 0,\n",
    "        \"sin_duplicados_pid_fecha\": dups == 0,\n",
    "        \"cluster_cubierto\": df.loc[df[cluster_col].isna(), \"product_id\"].nunique() == 0,\n",
    "    }\n",
    "    lines.append(\"\\n=== RESUMEN (OK=True) ===\")\n",
    "    for k, v in checks.items():\n",
    "        lines.append(f\"{k}: {bool(v)}\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# ---------- CLI (ignora flags extra de Jupyter) ----------\n",
    "def _parse_args() -> argparse.Namespace:\n",
    "    p = argparse.ArgumentParser(description=\"Validaci√≥n inicial del dataset de modelado (no escribe por defecto).\")\n",
    "    p.add_argument(\"--in\", dest=\"inp\", type=str, default=str(PROCESSED_DIR / \"subset_modelado.parquet\"),\n",
    "                   help=\"Ruta de entrada (PARQUET).\")\n",
    "    p.add_argument(\"--report\", dest=\"report\", type=str, default=\"\",\n",
    "                   help=\"Ruta TXT para volcar el reporte (opcional).\")\n",
    "    args, _ = p.parse_known_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "# ---------- Atajo para usar desde notebook ----------\n",
    "def run_validation(inp: str | Path = None, report: str | Path = None) -> str:\n",
    "    inp_path = Path(inp) if inp else (PROCESSED_DIR / \"subset_modelado.parquet\")\n",
    "    log.info(\"Leyendo: %s\", inp_path)\n",
    "    df = pd.read_parquet(inp_path)\n",
    "    log.info(\"Validando‚Ä¶\")\n",
    "    rep = validate_dataset(df)\n",
    "    if report:\n",
    "        report = Path(report)\n",
    "        report.parent.mkdir(parents=True, exist_ok=True)\n",
    "        report.write_text(rep, encoding=\"utf-8\")\n",
    "        log.info(\"Reporte guardado en: %s\", report)\n",
    "    return rep\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    args = _parse_args()\n",
    "    txt = run_validation(args.inp, args.report)\n",
    "    print(txt)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e3c84e",
   "metadata": {},
   "source": [
    "üìä **Resultados de la validaci√≥n inicial del dataset**\n",
    "\n",
    "La validaci√≥n aplicada sobre `subset_modelado.parquet` confirma que el dataset de partida es **consistente y apto para el modelado**.  \n",
    "\n",
    "**Principales resultados:**\n",
    "- ‚úîÔ∏è **Estructura completa**: se detectaron 25 columnas, incluyendo demanda, producto, cl√∫steres, precios y factores externos.  \n",
    "- ‚úîÔ∏è **Variable objetivo (`demand_final_noised`)**: sin valores nulos ni negativos.  \n",
    "- ‚úîÔ∏è **Integridad temporal**: fechas cubren el rango esperado (2022‚Äì2024), sin duplicados en la combinaci√≥n (`product_id`, `date`).  \n",
    "- ‚úîÔ∏è **Cobertura de cl√∫steres**: todos los productos tienen un cl√∫ster asignado.  \n",
    "- ‚úîÔ∏è **Factores de precio y externos**: columnas presentes y sin anomal√≠as graves.  \n",
    "\n",
    "**Implicaciones para el modelado:**\n",
    "- El dataset puede utilizarse directamente en la preparaci√≥n (fase 7.2) sin necesidad de limpieza adicional.  \n",
    "- La ausencia de nulos/duplicados evita sesgos en el backtesting y facilita la comparabilidad de m√©tricas.  \n",
    "- La cobertura de cl√∫steres garantiza que se pueda aplicar el enfoque de modelado **por cl√∫ster**, manteniendo consistencia metodol√≥gica.  \n",
    "\n",
    "> En conclusi√≥n, el dataset validado ofrece una **base s√≥lida y coherente** para iniciar la fase de modelado, reduciendo riesgos de errores estructurales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c73871",
   "metadata": {},
   "source": [
    "Adem√°s de la validaci√≥n principal, se cuenta con un **script espec√≠fico** (`check_outliers_clusters.py`)para auditar la coherencia de los *outliers* respecto a los cl√∫steres.\n",
    "\n",
    "**Objetivo.**  \n",
    "Comprobar que:\n",
    "- Los productos no marcados como *outliers* mantienen coherencia entre `cluster` y `__cluster__`.\n",
    "- Los productos marcados como *outliers* tienen un cl√∫ster asignado y se registra correctamente su distribuci√≥n.\n",
    "\n",
    "**Entradas.**\n",
    "- `data/processed/subset_modelado.parquet`\n",
    "\n",
    "**Salidas.**\n",
    "- Reporte en consola con:\n",
    "  - Distribuci√≥n de cl√∫steres.\n",
    "  - Coherencia `cluster` vs `__cluster__` en productos no-outlier.\n",
    "  - Resumen de productos outlier y cl√∫steres asignados.\n",
    "\n",
    "**Uso.**\n",
    "- CLI:\n",
    "  ```bash\n",
    "  python scripts/eda/check_outliers_clusters.py\n",
    "  python scripts/eda/check_outliers_clusters.py --report reports/outliers/summary_outliers_clusters.txt\n",
    "\n",
    "**Notas.**\n",
    "\n",
    "- Este script se considera una herramienta auxiliar para auditor√≠as puntuales.\n",
    "- Su ejecuci√≥n no es obligatoria en el pipeline, ya que la validaci√≥n principal (validacion_dataset_modelado.py) garantiza la integridad global.\n",
    "- Se recomienda utilizarlo si se desea revisar en detalle la trazabilidad de los outliers o documentar auditor√≠as espec√≠ficas.\n",
    "\n",
    "\n",
    "üìä **Resultados de la comprobaci√≥n auxiliar de outliers y cl√∫steres**\n",
    "Se ejecut√≥ el script `check_outliers_clusters.py` para verificar la coherencia de los *outliers* en relaci√≥n con los cl√∫steres.  \n",
    "\n",
    "**Principales hallazgos:**\n",
    "- ‚úîÔ∏è **Distribuci√≥n de cl√∫steres**: se identificaron 4 valores (0‚Äì3), con asignaci√≥n equilibrada y sin anomal√≠as.\n",
    "- ‚úîÔ∏è **No-outliers**: las columnas `cluster` y `__cluster__` son id√©nticas para todos los productos ‚Üí confirmada la coherencia.\n",
    "- ‚úîÔ∏è **Outliers**: 201 productos fueron marcados como outliers, y todos ellos fueron asignados de forma determinista al cl√∫ster mayoritario (`__cluster__ = 1`).\n",
    "\n",
    "**Implicaciones:**\n",
    "- La asignaci√≥n determinista a cl√∫ster 1 asegura que ning√∫n producto queda fuera del pipeline de modelado.\n",
    "- La validaci√≥n confirma que no existen inconsistencias entre `cluster` y `__cluster__` en los productos no-outlier.\n",
    "- La estrategia adoptada (incluir outliers como parte del cl√∫ster mayoritario) se mantiene v√°lida y no compromete la coherencia metodol√≥gica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1342d6f7",
   "metadata": {},
   "source": [
    "### **7.2. Preparaci√≥n de los datos para el modelado.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fdc3a6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Tras validar la integridad del dataset en el apartado 7.1, el siguiente paso consiste en **normalizar y depurar la estructura de datos** para que pueda ser utilizada directamente en el entrenamiento de los modelos.\n",
    "\n",
    "üéØ **Objetivo**.\n",
    "- Unificar nombres de columnas clave.\n",
    "- Eliminar duplicados y redundancias.\n",
    "- Definir expl√≠citamente el target y las features.\n",
    "- Generar un dataset limpio y homog√©neo que sirva como input est√°ndar para todos los modelos.\n",
    "\n",
    "üîÅ **Pasos realizados**.\n",
    "1. **Renombrado de columnas:**\n",
    "   - `__cluster__` ‚Üí `cluster_id`  \n",
    "   - `demand_final_noised` ‚Üí `sales_quantity`  \n",
    "\n",
    "2. **Eliminaci√≥n de duplicados:**\n",
    "   - Se descartan `cluster` y `__product_id__`, ya que eran copias redundantes de `__cluster__` y `product_id`.\n",
    "\n",
    "3. **Selecci√≥n de variables explicativas (features):**\n",
    "   - Precio: `precio_medio`, `price_virtual`, `price_factor_effective`, `demand_day_priceadj`.  \n",
    "   - Factores externos: `m_agosto_nonprice`, `m_competition`, `m_inflation`, `m_promo`, entre otros.  \n",
    "   - Outliers y trazabilidad: `is_outlier`, `tipo_outlier_year`, `decision_outlier_year`.  \n",
    "   - Identificadores y fecha: `product_id`, `cluster_id`, `date`.\n",
    "\n",
    "4. **Control de consistencia:**\n",
    "   - Verificaci√≥n de ausencia de duplicados en (`product_id`, `date`).  \n",
    "   - Confirmaci√≥n de que no existen valores nulos en la variable objetivo (`sales_quantity`).\n",
    "\n",
    "5. **Exportaci√≥n:**\n",
    "   - Se genera el dataset final `data/processed/dataset_modelado_ready.parquet`, que ser√° utilizado de manera uniforme en todos los experimentos de modelado.\n",
    "\n",
    "üß™ **Resultado**.\n",
    "El dataset preparado garantiza una **base coherente, sin ambig√ºedades ni redundancias**, y con una estructura estable que facilita:\n",
    "- La aplicaci√≥n consistente de modelos estad√≠sticos y de machine learning.  \n",
    "- La reproducibilidad de los experimentos (todos los modelos parten de la misma entrada).  \n",
    "- La trazabilidad de resultados (columnas de target y features claramente identificadas).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0110f34b",
   "metadata": {},
   "source": [
    "‚öôÔ∏è **Script: `preparacion_dataset_modelado.py`**\n",
    "\n",
    "üéØ **Objetivo.**  \n",
    "Normalizar y depurar el dataset de partida para que quede listo para el modelado, eliminando redundancias y asegurando que la estructura sea homog√©nea y estable.\n",
    "\n",
    "‚û°Ô∏è **Entradas.**\n",
    "- `data/processed/subset_modelado.parquet`\n",
    "\n",
    "‚¨ÖÔ∏è **Salidas.**\n",
    "- `data/processed/dataset_modelado_ready.parquet` (dataset final listo para modelado).\n",
    "\n",
    "üîÅ **Flujo de trabajo.**\n",
    "1. **Renombrado de columnas clave**  \n",
    "   - `__cluster__` ‚Üí `cluster_id`  \n",
    "   - `demand_final_noised` ‚Üí `sales_quantity`  \n",
    "\n",
    "2. **Eliminaci√≥n de columnas redundantes**  \n",
    "   - `cluster` (duplicado de `__cluster__`),  \n",
    "   - `__product_id__` (duplicado de `product_id`),  \n",
    "   - `demand_final_noiseds_adj` (columna auxiliar no utilizada).  \n",
    "\n",
    "3. **Normalizaci√≥n de tipos**  \n",
    "   - `date` ‚Üí formato datetime.  \n",
    "   - `product_id` ‚Üí string.  \n",
    "   - `cluster_id` ‚Üí entero (`int` o `Int64` si hay nulos).  \n",
    "\n",
    "4. **Control de duplicados y nulos**  \n",
    "   - Eliminaci√≥n de duplicados por (`product_id`, `date`).  \n",
    "   - Filtrado de posibles nulos en `sales_quantity`.  \n",
    "\n",
    "5. **Selecci√≥n de variables finales**  \n",
    "   - Identificadores y target: `product_id`, `date`, `cluster_id`, `sales_quantity`.  \n",
    "   - Features de precio, factores externos y trazabilidad (`precio_medio`, `price_virtual`, `m_promo`, `is_outlier`, etc.).  \n",
    "   - Ordenaci√≥n por (`product_id`, `date`).  \n",
    "\n",
    "6. **Exportaci√≥n**  \n",
    "   - Se guarda el dataset consolidado en `data/processed/dataset_modelado_ready.parquet`.  \n",
    "\n",
    "üìù **Notas.**\n",
    "- Este dataset es la **base de referencia para todos los modelos** de la Fase 7, evitando revalidaciones y asegurando consistencia.  \n",
    "- La eliminaci√≥n de redundancias y la normalizaci√≥n de tipos garantizan la trazabilidad y reproducibilidad de los resultados.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33ec834a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 12:29:42,828 | INFO | notebook.preparacion_dataset_modelado | Leyendo: C:\\Users\\crisr\\Desktop\\M√°ster Data Science & IA\\PROYECTO\\PFM2_Asistente_Compras_Inteligente\\data\\processed\\subset_modelado.parquet\n",
      "2025-09-08 12:29:43,464 | INFO | notebook.preparacion_dataset_modelado | Preparando dataset‚Ä¶\n",
      "2025-09-08 12:29:43,837 | INFO | notebook.preparacion_dataset_modelado | Eliminando columnas redundantes: ['cluster', '__product_id__', 'demand_final_noiseds_adj']\n",
      "2025-09-08 12:29:46,970 | INFO | notebook.preparacion_dataset_modelado | Guardado dataset listo para modelado en: C:\\Users\\crisr\\Desktop\\M√°ster Data Science & IA\\PROYECTO\\PFM2_Asistente_Compras_Inteligente\\data\\processed\\dataset_modelado_ready.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Script: preparacion_dataset_modelado.py\n",
    "# =============================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- Helper: localizar ra√≠z del repo (busca data/processed hacia arriba)\n",
    "def find_repo_root(start: Path | None = None) -> Path:\n",
    "    p = Path(start or Path.cwd()).resolve()\n",
    "    for parent in (p, *p.parents):\n",
    "        if (parent / \"data\" / \"processed\").exists():\n",
    "            return parent\n",
    "    return p  # fallback\n",
    "\n",
    "# ---------- Entorno (sirve para script y notebook)\n",
    "if \"__file__\" in globals():\n",
    "    _start = Path(__file__).resolve().parent\n",
    "    LOGGER_NAME = Path(__file__).stem\n",
    "else:\n",
    "    _start = Path.cwd()\n",
    "    LOGGER_NAME = \"notebook.preparacion_dataset_modelado\"\n",
    "\n",
    "ROOT_DIR = find_repo_root(_start)\n",
    "PROCESSED_DIR = ROOT_DIR / \"data\" / \"processed\"\n",
    "\n",
    "# ---------- Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    ")\n",
    "log = logging.getLogger(LOGGER_NAME)\n",
    "\n",
    "# ---------- Config ‚Äúsuave‚Äù: columnas a eliminar/renombrar/usar si existen\n",
    "RENAME_MAP = {\n",
    "    \"__cluster__\": \"cluster_id\",\n",
    "    \"demand_final_noised\": \"sales_quantity\",\n",
    "}\n",
    "DROP_CANDIDATES = [\n",
    "    \"cluster\",               # duplicado: nos quedamos con __cluster__ -> cluster_id\n",
    "    \"__product_id__\",        # duplicado de product_id\n",
    "    \"demand_final_noiseds_adj\",  # columna auxiliar que no aporta\n",
    "]\n",
    "# Features recomendadas (se usar√° la intersecci√≥n para evitar KeyError)\n",
    "FEATURES_RECOMENDADAS = [\n",
    "    # ids & fecha (estos los forzamos aparte)\n",
    "    # target -> sales_quantity (tras renombrado)\n",
    "    \"precio_medio\",\n",
    "    \"price_virtual\",\n",
    "    \"price_factor_effective\",\n",
    "    \"demand_day_priceadj\",\n",
    "    # factores externos\n",
    "    \"m_agosto_nonprice\",\n",
    "    \"m_competition\",\n",
    "    \"m_inflation\",\n",
    "    \"m_promo\",\n",
    "    # trazabilidad/outliers (opcionales, seg√∫n uso como ex√≥genas)\n",
    "    \"is_outlier\",\n",
    "    \"tipo_outlier_year\",\n",
    "    \"decision_outlier_year\",\n",
    "]\n",
    "\n",
    "# ---------- N√∫cleo ------------------------------------------------------------\n",
    "def prepare_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Aplica la preparaci√≥n para modelado y devuelve el DataFrame listo.\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1) Renombrados (solo si existen)\n",
    "    cols_a_renombrar = {c: n for c, n in RENAME_MAP.items() if c in df.columns}\n",
    "    df = df.rename(columns=cols_a_renombrar)\n",
    "\n",
    "    # Validaciones m√≠nimas\n",
    "    required = {\"product_id\", \"date\", \"cluster_id\", \"sales_quantity\"}\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Faltan columnas requeridas tras renombrado: {missing}\")\n",
    "\n",
    "    # 2) Eliminar columnas redundantes si existen\n",
    "    to_drop = [c for c in DROP_CANDIDATES if c in df.columns]\n",
    "    if to_drop:\n",
    "        log.info(\"Eliminando columnas redundantes: %s\", to_drop)\n",
    "        df = df.drop(columns=to_drop)\n",
    "\n",
    "    # 3) Normalizar tipos\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df[\"product_id\"] = df[\"product_id\"].astype(str)\n",
    "    # cluster_id como int (permitiendo nulos si los hubiera por seguridad)\n",
    "    if df[\"cluster_id\"].isna().any():\n",
    "        df[\"cluster_id\"] = df[\"cluster_id\"].astype(\"Int64\")\n",
    "    else:\n",
    "        df[\"cluster_id\"] = df[\"cluster_id\"].astype(int)\n",
    "\n",
    "    # 4) Control de duplicados por (product_id, date)\n",
    "    dups = df.duplicated([\"product_id\", \"date\"])\n",
    "    n_dup = int(dups.sum())\n",
    "    if n_dup > 0:\n",
    "        log.warning(\"Se detectaron %s duplicados (product_id, date). Se conservar√° el primero.\", n_dup)\n",
    "        df = df.loc[~dups].copy()\n",
    "\n",
    "    # 5) Verificaci√≥n de nulos en target\n",
    "    n_null_target = int(df[\"sales_quantity\"].isna().sum())\n",
    "    if n_null_target > 0:\n",
    "        log.warning(\"Se encontraron %s nulos en sales_quantity. Filtrando filas nulas.\", n_null_target)\n",
    "        df = df.loc[df[\"sales_quantity\"].notna()].copy()\n",
    "\n",
    "    # 6) Selecci√≥n de columnas finales (intersecci√≥n segura)\n",
    "    keep_base = [\"product_id\", \"date\", \"cluster_id\", \"sales_quantity\"]\n",
    "    keep_feats = [c for c in FEATURES_RECOMENDADAS if c in df.columns]\n",
    "    cols_finales = keep_base + keep_feats\n",
    "    df = df[cols_finales].sort_values([\"product_id\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# ---------- CLI ---------------------------------------------------------------\n",
    "def _parse_args() -> argparse.Namespace:\n",
    "    p = argparse.ArgumentParser(description=\"Preparaci√≥n del dataset para modelado.\")\n",
    "    p.add_argument(\"--in\",  dest=\"inp\",  type=str, default=str(PROCESSED_DIR / \"subset_modelado.parquet\"),\n",
    "                   help=\"Ruta de entrada (PARQUET).\")\n",
    "    p.add_argument(\"--out\", dest=\"outp\", type=str, default=str(PROCESSED_DIR / \"dataset_modelado_ready.parquet\"),\n",
    "                   help=\"Ruta de salida (PARQUET).\")\n",
    "    # Ignora flags de Jupyter si corre dentro de notebook\n",
    "    args, _ = p.parse_known_args()\n",
    "    return args\n",
    "\n",
    "def run_prep(inp: str | Path = None, outp: str | Path = None) -> str:\n",
    "    \"\"\"Atajo para usar desde notebook o como funci√≥n.\"\"\"\n",
    "    inp_path = Path(inp) if inp else (PROCESSED_DIR / \"subset_modelado.parquet\")\n",
    "    out_path = Path(outp) if outp else (PROCESSED_DIR / \"dataset_modelado_ready.parquet\")\n",
    "\n",
    "    log.info(\"Leyendo: %s\", inp_path)\n",
    "    df = pd.read_parquet(inp_path)\n",
    "\n",
    "    log.info(\"Preparando dataset‚Ä¶\")\n",
    "    df_ready = prepare_dataset(df)\n",
    "\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_ready.to_parquet(out_path, index=False)\n",
    "    log.info(\"Guardado dataset listo para modelado en: %s\", out_path)\n",
    "\n",
    "    return str(out_path)\n",
    "\n",
    "def main() -> None:\n",
    "    args = _parse_args()\n",
    "    run_prep(args.inp, args.outp)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51ddaef",
   "metadata": {},
   "source": [
    "‚úÖ **Verificaci√≥n post-transformaci√≥n del dataset listo para modelado.**\n",
    "\n",
    "Tras la preparaci√≥n del dataset (`dataset_modelado_ready.parquet`), se realiza una verificaci√≥n ligera para asegurar que la transformaci√≥n **no ha introducido errores** y que la estructura final es apta para los modelos.\n",
    "\n",
    "**Qu√© se comprueba:**\n",
    "- **Cobertura temporal:** las fechas abarcan el rango esperado (2022-01-01 ‚Üí 2024-12-31).\n",
    "- **Target (`sales_quantity`):** sin valores **nulos** ni **negativos**.  \n",
    "  > No se validan los **ceros** porque son coherentes con d√≠as sin ventas.\n",
    "- **Identificador (`product_id`):** sin nulos, sin valores ‚Äú0‚Äù ni cadenas vac√≠as.\n",
    "- **Duplicados:** no existen duplicados en la combinaci√≥n (`product_id`, `date`).\n",
    "- **Cl√∫ster (`cluster_id`):** sin valores nulos y con valores dentro del rango esperado.\n",
    "\n",
    "**Por qu√© es necesaria esta verificaci√≥n:**\n",
    "- Cada transformaci√≥n (renombrados, drops, normalizaci√≥n) puede introducir errores de forma accidental.\n",
    "- Esta comprobaci√≥n act√∫a como **‚Äúpost-check‚Äù** del bloque 7.2 y da garant√≠as de que el dataset preparado mantiene la **integridad y consistencia** exigidas por el pipeline de modelado.\n",
    "\n",
    "> Esta verificaci√≥n es **operativa** y se mantiene en el **notebook** (no forma parte del pipeline en scripts) para agilizar el trabajo exploratorio y la defensa del proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57382c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Cobertura temporal ===\n",
      "Fecha m√≠nima: 2022-01-01 00:00:00\n",
      "Fecha m√°xima: 2024-12-31 00:00:00\n",
      "Cobertura dentro del rango esperado: True\n",
      "\n",
      "=== sales_quantity (target) ===\n",
      "Nulos: 0\n",
      "Negativos: 0\n",
      "Target OK (sin nulos ni negativos): True\n",
      "\n",
      "=== product_id ===\n",
      "Nulos: 0\n",
      "Valores '0': 0\n",
      "Vac√≠os (''): 0\n",
      "√önicos: 3596\n",
      "product_id OK (no nulos/0/vac√≠os): True\n",
      "\n",
      "=== Duplicados (product_id, date) ===\n",
      "Duplicados: 0\n",
      "Sin duplicados pid+date: True\n",
      "\n",
      "=== cluster_id ===\n",
      "Nulos: 0\n",
      "Valores √∫nicos: [0, 1, 2, 3]\n",
      "cluster_id OK (sin nulos): True\n",
      "\n",
      "=== Resumen (OK=True) ===\n",
      "cobertura_temporal_ok: True\n",
      "target_ok: True\n",
      "product_id_ok: True\n",
      "sin_duplicados_pid_date: True\n",
      "cluster_ok: True\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Script: preparacion_dataset_modelado.py\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# === Config ===\n",
    "PATH_READY = Path(r\"C:\\Users\\crisr\\Desktop\\M√°ster Data Science & IA\\PROYECTO\\PFM2_Asistente_Compras_Inteligente\\data\\processed\\dataset_modelado_ready.parquet\")\n",
    "FECHA_MIN_ESPERADA = pd.Timestamp(\"2022-01-01\")\n",
    "FECHA_MAX_ESPERADA = pd.Timestamp(\"2024-12-31\")\n",
    "\n",
    "# === Carga ===\n",
    "df = pd.read_parquet(PATH_READY)\n",
    "\n",
    "# Asegurar tipos\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df[\"product_id\"] = df[\"product_id\"].astype(str)\n",
    "\n",
    "print(\"=== Cobertura temporal ===\")\n",
    "print(\"Fecha m√≠nima:\", df[\"date\"].min())\n",
    "print(\"Fecha m√°xima:\", df[\"date\"].max())\n",
    "cobertura_ok = (df[\"date\"].min() <= FECHA_MIN_ESPERADA) and (df[\"date\"].max() >= FECHA_MAX_ESPERADA)\n",
    "print(\"Cobertura dentro del rango esperado:\", cobertura_ok)\n",
    "\n",
    "print(\"\\n=== sales_quantity (target) ===\")\n",
    "print(\"Nulos:\", int(df[\"sales_quantity\"].isna().sum()))\n",
    "print(\"Negativos:\", int((df[\"sales_quantity\"] < 0).sum()))\n",
    "target_ok = (df[\"sales_quantity\"].isna().sum() == 0) and ((df[\"sales_quantity\"] < 0).sum() == 0)\n",
    "print(\"Target OK (sin nulos ni negativos):\", target_ok)\n",
    "\n",
    "print(\"\\n=== product_id ===\")\n",
    "print(\"Nulos:\", int(df[\"product_id\"].isna().sum()))\n",
    "print(\"Valores '0':\", int((df[\"product_id\"] == \"0\").sum()))\n",
    "print(\"Vac√≠os (''):\", int((df[\"product_id\"].str.len() == 0).sum()))\n",
    "print(\"√önicos:\", df[\"product_id\"].nunique())\n",
    "pid_ok = (df[\"product_id\"].isna().sum() == 0) and ((df[\"product_id\"] == \"0\").sum() == 0) and ((df[\"product_id\"].str.len() == 0).sum() == 0)\n",
    "print(\"product_id OK (no nulos/0/vac√≠os):\", pid_ok)\n",
    "\n",
    "print(\"\\n=== Duplicados (product_id, date) ===\")\n",
    "dup_count = int(df.duplicated([\"product_id\", \"date\"]).sum())\n",
    "print(\"Duplicados:\", dup_count)\n",
    "dups_ok = dup_count == 0\n",
    "print(\"Sin duplicados pid+date:\", dups_ok)\n",
    "\n",
    "print(\"\\n=== cluster_id ===\")\n",
    "print(\"Nulos:\", int(df[\"cluster_id\"].isna().sum()))\n",
    "vals = sorted(pd.Series(df[\"cluster_id\"].dropna().unique()).tolist())\n",
    "print(\"Valores √∫nicos:\", vals)\n",
    "cluster_ok = df[\"cluster_id\"].isna().sum() == 0\n",
    "print(\"cluster_id OK (sin nulos):\", cluster_ok)\n",
    "\n",
    "print(\"\\n=== Resumen (OK=True) ===\")\n",
    "checks = {\n",
    "    \"cobertura_temporal_ok\": cobertura_ok,\n",
    "    \"target_ok\": target_ok,\n",
    "    \"product_id_ok\": pid_ok,\n",
    "    \"sin_duplicados_pid_date\": dups_ok,\n",
    "    \"cluster_ok\": cluster_ok,\n",
    "}\n",
    "for k, v in checks.items():\n",
    "    print(f\"{k}: {bool(v)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f11301",
   "metadata": {},
   "source": [
    "üìä **Resultados de la verificaci√≥n post-transformaci√≥n**.\n",
    "\n",
    "La verificaci√≥n realizada sobre el dataset `dataset_modelado_ready.parquet` confirma que la transformaci√≥n no introdujo errores y que la estructura final es **coherente y apta para el modelado**.\n",
    "\n",
    "**Hallazgos principales:**\n",
    "- ‚úîÔ∏è **Cobertura temporal completa:** fechas desde 2022-01-01 hasta 2024-12-31.  \n",
    "- ‚úîÔ∏è **Target (`sales_quantity`):** sin nulos ni valores negativos. Los ceros se mantienen como representaci√≥n v√°lida de d√≠as sin ventas.  \n",
    "- ‚úîÔ∏è **Product_ID:** sin nulos, sin valores inv√°lidos (0 o cadenas vac√≠as). Se identifican 3.596 productos √∫nicos.  \n",
    "- ‚úîÔ∏è **Duplicados:** no existen duplicados en la combinaci√≥n (`product_id`, `date`).  \n",
    "- ‚úîÔ∏è **Cluster_ID:** todos los productos tienen cl√∫ster asignado (0‚Äì3), sin nulos ni valores fuera de rango.  \n",
    "\n",
    "> **Conclusi√≥n:**  \n",
    "El dataset preparado conserva la integridad y consistencia requeridas.  \n",
    "Esto asegura que el archivo `dataset_modelado_ready.parquet` puede utilizarse como **input √∫nico y estable** en todos los experimentos de la Fase 7, garantizando trazabilidad, reproducibilidad y ausencia de sesgos estructurales."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
