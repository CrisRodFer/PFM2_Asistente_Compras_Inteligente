{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd54eafc",
   "metadata": {},
   "source": [
    "# ITERACIONES: **2.5. Clustering de productos: comparativa de técnicas de análisis clúster.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf3fad2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Propósito.**  \n",
    "Validar la robustez del clustering aplicado al catálogo de productos comparándolo con técnicas alternativas. El objetivo no es sustituir el método actual por defecto, sino **comprobar si K-Means es adecuado** para nuestros datos o si existe una alternativa que aporte **mejor separación/cohesión** y/o **mejor interpretabilidad**.\n",
    "\n",
    "\n",
    "## Datos de entrada\n",
    "\n",
    "- **Dataset**: `data/processed/productos_features_norm.csv`  \n",
    "  (variables numéricas ya estandarizadas: `d_total`, `d_media`, `d_std`, `cv`, `p95`, `mediana`, `precio_medio`, `PC1`, `PC2`, `PC3`; `Product_ID` como identificador).\n",
    "- Razón de uso: todas las técnicas se benefician de trabajar en la misma escala; garantiza comparabilidad.\n",
    "\n",
    "\n",
    "\n",
    "## Metodología de comparación\n",
    "\n",
    "1. **Baseline (Iteración 0): K-Means (k=4)**  \n",
    "   Partimos de los resultados ya obtenidos (inercia y silhouette vs. *k*, asignación final). Esta iteración sirve de referencia para la comparativa.\n",
    "\n",
    "2. **Técnicas alternativas**\n",
    "   - **Iteración 1: Agglomerative Clustering (jerárquico)**  \n",
    "     Enlace *ward* sobre distancia euclidiana (coherente con datos normalizados). Se evaluarán varios *k*.\n",
    "   - **Iteración 2: Gaussian Mixture Models (GMM)**  \n",
    "     Enfoque probabilístico; permite solapamiento entre clusters. Se evaluarán varios *k* y tipos de covarianzas.\n",
    "   - **Iteración 3: DBSCAN**  \n",
    "     Clustering basado en densidad; útil para detectar outliers o grupos no esféricos (sin fijar *k*). Se explorarán pares `(eps, min_samples)` razonables.\n",
    "\n",
    "3. **Métricas de evaluación (internas)**\n",
    "   - **Silhouette score** (↑ mejor): cohesión intra-cluster y separación inter-cluster.\n",
    "   - **Davies-Bouldin index** (↓ mejor): ratio de dispersión intra + distancia inter.\n",
    "   - **Tamaños de cluster**: evitar clusters triviales (muy pequeños) o dominantes excesivos.\n",
    "   - (Opcional) **Calinski-Harabasz** (↑ mejor): varianza inter/intra.\n",
    "\n",
    "> Nota: La comparación se centrará en métricas internas (no hay etiquetas “verdaderas”). La **validación de negocio** e **interpretabilidad** se hará en la fase de conclusiones.\n",
    "\n",
    "\n",
    "\n",
    "## Estructura del cuaderno\n",
    "\n",
    "- **Iteración 0 — Baseline K-Means**  \n",
    "  Resumen de resultados (k seleccionado, silhouette, distribución de tamaños).\n",
    "- **Iteración 1 — Agglomerative**  \n",
    "  Exploración por *k*: métricas + distribución de tamaños.\n",
    "- **Iteración 2 — GMM**  \n",
    "  Exploración por *k* y covarianzas: métricas + distribución de tamaños.\n",
    "- **Iteración 3 — DBSCAN**  \n",
    "  Exploración de `(eps, min_samples)`: métricas + tamaños (incluyendo ruido).\n",
    "- **Tabla comparativa final**  \n",
    "  Resumen de todas las técnicas/hiperparámetros con métricas lado a lado.\n",
    "- **Conclusiones**  \n",
    "  - ¿K-Means es suficiente o hay alternativa superior?  \n",
    "  - ¿Los resultados son estables y útiles para negocio?  \n",
    "  - **Decisión**: técnica seleccionada para el proyecto.  \n",
    "  - Si procede, **siguientes pasos** (crear script definitivo con la técnica elegida).\n",
    "\n",
    "\n",
    "## Criterios de decisión\n",
    "\n",
    "1. **Métricas internas**: mejor *silhouette*, menor *Davies-Bouldin* (y, opcionalmente, mayor *Calinski-Harabasz*).  \n",
    "2. **Estructura razonable**: tamaños de cluster no extremos y sin clusters “vacíos”.  \n",
    "3. **Parquedad**: preferencia por modelos simples si el rendimiento es similar.  \n",
    "\n",
    "\n",
    "## Resultados esperados\n",
    "\n",
    "- Evidencia cuantitativa (gráficas/tablas) que **confirme o refute** la idoneidad de K-Means.  \n",
    "- Recomendación final y, en su caso, **técnica definitiva** a implementar en script de producción.\n",
    "\n",
    "> Este cuaderno no genera scripts por iteración. Si se decide cambiar la técnica, se implementará un **único script definitivo** con el algoritmo seleccionado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3a8170",
   "metadata": {},
   "source": [
    "## 1.  **Iteración 0.** \n",
    "#### ***Baseline K-Means***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81011493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 13:41:31 | INFO | ARGS -> in=C:\\Users\\crisr\\Desktop\\Máster Data Science & IA\\PROYECTO\\PFM2_Asistente_Compras_Inteligente\\data\\processed\\productos_features_norm.csv | out=C:\\Users\\crisr\\Desktop\\Máster Data Science & IA\\PROYECTO\\PFM2_Asistente_Compras_Inteligente\\data\\processed\\productos_clusters.csv | k=[3..10] | force_k=None | sil_sample=5000 | seed=42 | n_init=20\n",
      "2025-08-28 13:41:31 | INFO | Cargando features normalizadas: C:\\Users\\crisr\\Desktop\\Máster Data Science & IA\\PROYECTO\\PFM2_Asistente_Compras_Inteligente\\data\\processed\\productos_features_norm.csv\n",
      "2025-08-28 13:41:31 | INFO | Dimensiones: n=5938, d=10\n",
      "2025-08-28 13:41:31 | INFO | [Exploración] Ajustando KMeans con k=3 ...\n",
      "2025-08-28 13:41:34 | INFO | [Exploración] Ajustando KMeans con k=4 ...\n",
      "2025-08-28 13:41:34 | INFO | [Exploración] Ajustando KMeans con k=5 ...\n",
      "2025-08-28 13:41:34 | INFO | [Exploración] Ajustando KMeans con k=6 ...\n",
      "2025-08-28 13:41:35 | INFO | [Exploración] Ajustando KMeans con k=7 ...\n",
      "2025-08-28 13:41:35 | INFO | [Exploración] Ajustando KMeans con k=8 ...\n",
      "2025-08-28 13:41:36 | INFO | [Exploración] Ajustando KMeans con k=9 ...\n",
      "2025-08-28 13:41:36 | INFO | [Exploración] Ajustando KMeans con k=10 ...\n",
      "2025-08-28 13:41:37 | INFO | Guardado: C:\\Users\\crisr\\Desktop\\Máster Data Science & IA\\PROYECTO\\PFM2_Asistente_Compras_Inteligente\\reports\\inercia_vs_k.csv\n",
      "2025-08-28 13:41:37 | INFO | Guardado: C:\\Users\\crisr\\Desktop\\Máster Data Science & IA\\PROYECTO\\PFM2_Asistente_Compras_Inteligente\\reports\\silhouette_vs_k.csv\n",
      "2025-08-28 13:41:37 | INFO | Selección automática por silhouette: k=4 (silhouette=0.3226)\n",
      "2025-08-28 13:41:37 | INFO | Ajustando modelo final KMeans con k=4 ...\n",
      "2025-08-28 13:41:37 | INFO | === VALIDACIÓN CLUSTERING ===\n",
      "2025-08-28 13:41:37 | INFO | k final: 4\n",
      "2025-08-28 13:41:37 | INFO | Silhouette (final): 0.3226\n",
      "2025-08-28 13:41:37 | INFO | Tamaños de cluster: {0: 210, 1: 1233, 2: 3394, 3: 1101} (min=210)\n",
      "2025-08-28 13:41:37 | INFO | Guardado dataset con clusters: C:\\Users\\crisr\\Desktop\\Máster Data Science & IA\\PROYECTO\\PFM2_Asistente_Compras_Inteligente\\data\\processed\\productos_clusters.csv (filas=5938, cols=12)\n",
      "2025-08-28 13:41:37 | INFO | Proceso finalizado. k_final=4 | silhouette_final=0.3225507935512098\n",
      "2025-08-28 13:41:37 | INFO | Rutas: {'clusters': 'C:\\\\Users\\\\crisr\\\\Desktop\\\\Máster Data Science & IA\\\\PROYECTO\\\\PFM2_Asistente_Compras_Inteligente\\\\data\\\\processed\\\\productos_clusters.csv', 'inercia_vs_k': 'C:\\\\Users\\\\crisr\\\\Desktop\\\\Máster Data Science & IA\\\\PROYECTO\\\\PFM2_Asistente_Compras_Inteligente\\\\reports\\\\inercia_vs_k.csv', 'silhouette_vs_k': 'C:\\\\Users\\\\crisr\\\\Desktop\\\\Máster Data Science & IA\\\\PROYECTO\\\\PFM2_Asistente_Compras_Inteligente\\\\reports\\\\silhouette_vs_k.csv'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Script: clustering_productos.py\n",
    "# ============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import sys, argparse, logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# --------------------------- Raíz del proyecto ------------------------------\n",
    "def _detect_root_when_no_file():\n",
    "    here = Path().resolve()\n",
    "    for p in [here, *here.parents]:\n",
    "        if (p / \"data\").is_dir():\n",
    "            return p\n",
    "    return here\n",
    "\n",
    "if \"__file__\" in globals():\n",
    "    ROOT_DIR = Path(__file__).resolve().parents[2]\n",
    "else:\n",
    "    ROOT_DIR = _detect_root_when_no_file()\n",
    "\n",
    "DATA_DIR      = ROOT_DIR / \"data\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "REPORTS_DIR   = ROOT_DIR / \"reports\"\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --------------------------------- Logging ----------------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "logger = logging.getLogger(\"clustering_productos\")\n",
    "\n",
    "# --- Parche Jupyter: elimina --f=... del kernel para argparse ---------------\n",
    "if \"ipykernel\" in sys.modules or \"IPython\" in sys.modules:\n",
    "    sys.argv = [sys.argv[0]]\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# ------------------------------- Utilidades ---------------------------------\n",
    "NUM_COLS = [\n",
    "    \"d_total\", \"d_media\", \"d_std\", \"cv\", \"p95\", \"mediana\",\n",
    "    \"precio_medio\", \"PC1\", \"PC2\", \"PC3\"\n",
    "]\n",
    "\n",
    "def _check_columns(df: pd.DataFrame, cols: list[str]):\n",
    "    faltan = [c for c in cols if c not in df.columns]\n",
    "    if faltan:\n",
    "        raise KeyError(f\"Faltan columnas en el dataset de entrada: {faltan}\")\n",
    "\n",
    "def _sample_for_silhouette(X: np.ndarray, max_n: int, random_state: int = 42):\n",
    "    n = X.shape[0]\n",
    "    if n <= max_n:\n",
    "        return X, np.arange(n)\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    idx = rng.choice(n, size=max_n, replace=False)\n",
    "    return X[idx], idx\n",
    "\n",
    "# ---------------------------------- Core ------------------------------------\n",
    "def explorar_y_clusterizar(in_path: Path,\n",
    "                           out_path: Path,\n",
    "                           k_min: int = 3,\n",
    "                           k_max: int = 10,\n",
    "                           force_k: int | None = None,\n",
    "                           sil_sample: int = 5000,\n",
    "                           random_state: int = 42,\n",
    "                           n_init: int = 20):\n",
    "\n",
    "    # 1) Cargar dataset normalizado\n",
    "    in_path  = Path(in_path)\n",
    "    out_path = Path(out_path)\n",
    "    logger.info(f\"Cargando features normalizadas: {in_path}\")\n",
    "    df = pd.read_csv(in_path)\n",
    "\n",
    "    if \"Product_ID\" not in df.columns:\n",
    "        raise KeyError(\"Se requiere columna 'Product_ID' en el dataset de entrada.\")\n",
    "    _check_columns(df, NUM_COLS)\n",
    "\n",
    "    X = df[NUM_COLS].astype(float).values\n",
    "    n, d = X.shape\n",
    "    logger.info(f\"Dimensiones: n={n}, d={d}\")\n",
    "\n",
    "    # 2) Explorar rango de k (si no se fuerza)\n",
    "    ks = list(range(max(2, k_min), max(k_min, k_max) + 1))\n",
    "    res_inercia = []\n",
    "    res_sil = []\n",
    "\n",
    "    for k in ks:\n",
    "        logger.info(f\"[Exploración] Ajustando KMeans con k={k} ...\")\n",
    "        km = KMeans(n_clusters=k, random_state=random_state, n_init=n_init)\n",
    "        labels = km.fit_predict(X)\n",
    "        inertia = float(km.inertia_)\n",
    "        res_inercia.append({\"k\": k, \"inercia\": inertia})\n",
    "\n",
    "        # Silhouette (requiere k>=2, ya garantizado) — muestreo opcional por eficiencia\n",
    "        X_sil, idx_sil = _sample_for_silhouette(X, max_n=sil_sample, random_state=random_state)\n",
    "        lab_sil = labels[idx_sil]\n",
    "        try:\n",
    "            sil = float(silhouette_score(X_sil, lab_sil, metric=\"euclidean\"))\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Silhouette falló para k={k}: {e}\")\n",
    "            sil = np.nan\n",
    "        res_sil.append({\"k\": k, \"silhouette\": sil})\n",
    "\n",
    "    df_inercia = pd.DataFrame(res_inercia)\n",
    "    df_sil = pd.DataFrame(res_sil)\n",
    "\n",
    "    # Guardar reportes\n",
    "    path_inercia = REPORTS_DIR / \"inercia_vs_k.csv\"\n",
    "    path_sil = REPORTS_DIR / \"silhouette_vs_k.csv\"\n",
    "    df_inercia.to_csv(path_inercia, index=False)\n",
    "    df_sil.to_csv(path_sil, index=False)\n",
    "    logger.info(f\"Guardado: {path_inercia}\")\n",
    "    logger.info(f\"Guardado: {path_sil}\")\n",
    "\n",
    "    # 3) Selección de k\n",
    "    if force_k is not None:\n",
    "        best_k = int(force_k)\n",
    "        logger.info(f\"Usando k forzado por CLI: k={best_k}\")\n",
    "    else:\n",
    "        # Elegir k por máximo silhouette (ignorando NaN); si empate, el menor k\n",
    "        df_sil_valid = df_sil.dropna(subset=[\"silhouette\"])\n",
    "        if df_sil_valid.empty:\n",
    "            # fallback: si no hay silhouette válido, usar punto medio del rango\n",
    "            best_k = int(np.median(ks))\n",
    "            logger.warning(f\"No se pudo calcular silhouette; usando k={best_k} (mediana del rango).\")\n",
    "        else:\n",
    "            max_sil = df_sil_valid[\"silhouette\"].max()\n",
    "            candidatos = df_sil_valid.loc[df_sil_valid[\"silhouette\"] == max_sil, \"k\"].tolist()\n",
    "            best_k = min(candidatos)\n",
    "            logger.info(f\"Selección automática por silhouette: k={best_k} (silhouette={max_sil:.4f})\")\n",
    "\n",
    "    # 4) Modelo final con best_k\n",
    "    logger.info(f\"Ajustando modelo final KMeans con k={best_k} ...\")\n",
    "    km_final = KMeans(n_clusters=best_k, random_state=random_state, n_init=n_init)\n",
    "    labels_final = km_final.fit_predict(X)\n",
    "\n",
    "    # Validación silhouette final (completo o muestreado si es muy grande)\n",
    "    X_sil_final, idx_sil_final = _sample_for_silhouette(X, max_n=sil_sample, random_state=random_state)\n",
    "    lab_sil_final = labels_final[idx_sil_final]\n",
    "    try:\n",
    "        sil_final = float(silhouette_score(X_sil_final, lab_sil_final, metric=\"euclidean\"))\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Silhouette final falló para k={best_k}: {e}\")\n",
    "        sil_final = np.nan\n",
    "\n",
    "    # 5) Distribución de tamaños de cluster\n",
    "    _, counts = np.unique(labels_final, return_counts=True)\n",
    "    dist_sizes = {int(i): int(c) for i, c in enumerate(counts)}\n",
    "    min_size = counts.min()\n",
    "    logger.info(\"=== VALIDACIÓN CLUSTERING ===\")\n",
    "    logger.info(f\"k final: {best_k}\")\n",
    "    logger.info(f\"Silhouette (final): {sil_final:.4f}\" if not np.isnan(sil_final) else \"Silhouette (final): NaN\")\n",
    "    logger.info(f\"Tamaños de cluster: {dist_sizes} (min={min_size})\")\n",
    "\n",
    "    # 6) Export asignaciones\n",
    "    df_clusters = df.copy()\n",
    "    df_clusters[\"Cluster\"] = labels_final\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_clusters.to_csv(out_path, index=False)\n",
    "    logger.info(f\"Guardado dataset con clusters: {out_path} (filas={len(df_clusters)}, cols={df_clusters.shape[1]})\")\n",
    "\n",
    "    # 7) Devolver info clave\n",
    "    return {\n",
    "        \"k_final\": best_k,\n",
    "        \"silhouette_final\": sil_final,\n",
    "        \"sizes\": dist_sizes,\n",
    "        \"paths\": {\n",
    "            \"clusters\": str(out_path),\n",
    "            \"inercia_vs_k\": str(path_inercia),\n",
    "            \"silhouette_vs_k\": str(path_sil),\n",
    "        },\n",
    "    }\n",
    "\n",
    "# ------------------------------------ CLI -----------------------------------\n",
    "def parse_args(argv=None):\n",
    "    p = argparse.ArgumentParser(description=\"Clustering de productos (K-Means) con exploración de k e informes.\")\n",
    "    p.add_argument(\"--in\",       dest=\"inp\",  type=str, default=str(PROCESSED_DIR / \"productos_features_norm.csv\"))\n",
    "    p.add_argument(\"--out\",      dest=\"outp\", type=str, default=str(PROCESSED_DIR / \"productos_clusters.csv\"))\n",
    "    p.add_argument(\"--k-min\",    dest=\"kmin\", type=int, default=3)\n",
    "    p.add_argument(\"--k-max\",    dest=\"kmax\", type=int, default=10)\n",
    "    p.add_argument(\"--force-k\",  dest=\"kforce\", type=int, default=None, help=\"Forzar k concreto. Si se indica, salta la selección automática.\")\n",
    "    p.add_argument(\"--sil-sample\", dest=\"silsample\", type=int, default=5000,\n",
    "                   help=\"Máximo de observaciones para calcular silhouette (muestreo aleatorio si N>valor).\")\n",
    "    p.add_argument(\"--seed\",     dest=\"seed\", type=int, default=42)\n",
    "    p.add_argument(\"--n-init\",   dest=\"ninit\", type=int, default=20)\n",
    "\n",
    "    if argv is None and (\"ipykernel\" in sys.modules or \"IPython\" in sys.modules):\n",
    "        argv = []\n",
    "\n",
    "    args, _ = p.parse_known_args(argv)\n",
    "    logger.info(\"ARGS -> in=%s | out=%s | k=[%d..%d] | force_k=%s | sil_sample=%d | seed=%d | n_init=%d\",\n",
    "                args.inp, args.outp, args.kmin, args.kmax, str(args.kforce), args.silsample, args.seed, args.ninit)\n",
    "    return args\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    try:\n",
    "        info = explorar_y_clusterizar(\n",
    "            in_path=Path(args.inp),\n",
    "            out_path=Path(args.outp),\n",
    "            k_min=args.kmin,\n",
    "            k_max=args.kmax,\n",
    "            force_k=args.kforce,\n",
    "            sil_sample=args.silsample,\n",
    "            random_state=args.seed,\n",
    "            n_init=args.ninit\n",
    "        )\n",
    "        logger.info(\"Proceso finalizado. k_final=%s | silhouette_final=%s\", info[\"k_final\"], info[\"silhouette_final\"])\n",
    "        logger.info(\"Rutas: %s\", info[\"paths\"])\n",
    "    except Exception as e:\n",
    "        logging.exception(f\"Error en clustering: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4d45b8",
   "metadata": {},
   "source": [
    "\n",
    "## 2.  **Iteración 1.** \n",
    "#### ***Agglomerative Clustering***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77981140",
   "metadata": {},
   "source": [
    "\n",
    "🔍 **En qué consiste la técnica**\n",
    "El **clustering aglomerativo** es un método jerárquico que parte de la idea opuesta a K-Means:  \n",
    "- Cada producto comienza en su propio cluster.  \n",
    "- En cada paso, se fusionan los dos clusters más similares según una métrica de distancia y un criterio de enlace.  \n",
    "- El proceso continúa hasta formar el número deseado de clusters (*k*).  \n",
    "\n",
    "El criterio utilizado en este caso es el **enlace Ward**, que fusiona clusters minimizando la varianza interna. Esto lo hace especialmente adecuado para datos previamente normalizados.\n",
    "\n",
    "\n",
    "🧩 **Cómo funciona el script**\n",
    "El script `clustering_productos_agglomerative.py` ejecuta los siguientes pasos:\n",
    "\n",
    "1. **Entrada**  \n",
    "   - Lee el dataset `productos_features_norm.csv`.  \n",
    "   - Utiliza las variables numéricas normalizadas como base para el clustering.\n",
    "\n",
    "2. **Exploración de *k***  \n",
    "   - Ajusta modelos de Agglomerative Clustering para distintos valores de *k* en un rango definido (por defecto 3 a 10).  \n",
    "   - Calcula métricas internas de validación para cada *k*:  \n",
    "     - **Silhouette score** (↑ mejor).  \n",
    "     - **Davies–Bouldin index (DBI)** (↓ mejor).  \n",
    "     - **Calinski–Harabasz (CH)** (↑ mejor).\n",
    "\n",
    "3. **Selección del número de clusters**  \n",
    "   - Elige el *k* con mejor **Silhouette score** (o se puede forzar un valor específico desde CLI).  \n",
    "\n",
    "4. **Entrenamiento final y salida**  \n",
    "   - Ajusta el modelo final con el *k* seleccionado.  \n",
    "   - Asigna un cluster a cada producto, añadiendo la columna `Cluster_Agglo`.  \n",
    "   - Exporta resultados y reportes:  \n",
    "     - `productos_clusters_agglom.csv` con las asignaciones.  \n",
    "     - `silhouette_vs_k_agglom.csv`, `davies_bouldin_vs_k_agglom.csv` y `calinski_harabasz_vs_k_agglom.csv` con métricas de exploración.  \n",
    "\n",
    "\n",
    "🎯 **Objetivo de esta iteración**\n",
    "Comparar el rendimiento de **Agglomerative Clustering** frente al baseline de K-Means:  \n",
    "- Ver si obtiene clusters con mejor cohesión/separación (Silhouette ↑, DBI ↓).  \n",
    "- Evaluar la estabilidad y distribución de tamaños.  \n",
    "- Comprobar si la estructura jerárquica revela patrones diferentes o más interpretables que K-Means.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef4572c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 16:02:30 | INFO | ARGS -> in=C:\\Users\\crisr\\Desktop\\Máster Data Science & IA\\PROYECTO\\PFM2_Asistente_Compras_Inteligente\\data\\processed\\productos_features_norm.csv | out=C:\\Users\\crisr\\Desktop\\Máster Data Science & IA\\PROYECTO\\PFM2_Asistente_Compras_Inteligente\\data\\processed\\productos_clusters_agglom.csv | k=[3..10] | force_k=None | linkage=ward | metric=euclidean | sil_sample=7000\n",
      "2025-08-28 16:02:30 | INFO | Cargando features normalizadas: C:\\Users\\crisr\\Desktop\\Máster Data Science & IA\\PROYECTO\\PFM2_Asistente_Compras_Inteligente\\data\\processed\\productos_features_norm.csv\n",
      "2025-08-28 16:02:30 | INFO | [Exploración] Agglomerative (linkage=ward, metric=euclidean) con k=3\n",
      "2025-08-28 16:02:31 | INFO | [Exploración] Agglomerative (linkage=ward, metric=euclidean) con k=4\n",
      "2025-08-28 16:02:32 | INFO | [Exploración] Agglomerative (linkage=ward, metric=euclidean) con k=5\n",
      "2025-08-28 16:02:33 | INFO | [Exploración] Agglomerative (linkage=ward, metric=euclidean) con k=6\n",
      "2025-08-28 16:02:34 | INFO | [Exploración] Agglomerative (linkage=ward, metric=euclidean) con k=7\n",
      "2025-08-28 16:02:35 | INFO | [Exploración] Agglomerative (linkage=ward, metric=euclidean) con k=8\n",
      "2025-08-28 16:02:36 | INFO | [Exploración] Agglomerative (linkage=ward, metric=euclidean) con k=9\n",
      "2025-08-28 16:02:37 | INFO | [Exploración] Agglomerative (linkage=ward, metric=euclidean) con k=10\n",
      "2025-08-28 16:02:38 | INFO | Guardado: C:\\Users\\crisr\\Desktop\\Máster Data Science & IA\\PROYECTO\\PFM2_Asistente_Compras_Inteligente\\reports\\silhouette_vs_k_agglom.csv\n",
      "2025-08-28 16:02:38 | INFO | Guardado: C:\\Users\\crisr\\Desktop\\Máster Data Science & IA\\PROYECTO\\PFM2_Asistente_Compras_Inteligente\\reports\\davies_bouldin_vs_k_agglom.csv\n",
      "2025-08-28 16:02:38 | INFO | Guardado: C:\\Users\\crisr\\Desktop\\Máster Data Science & IA\\PROYECTO\\PFM2_Asistente_Compras_Inteligente\\reports\\calinski_harabasz_vs_k_agglom.csv\n",
      "2025-08-28 16:02:38 | INFO | Selección automática por silhouette: k=4 (silhouette=0.3024)\n",
      "2025-08-28 16:02:39 | INFO | === VALIDACIÓN CLUSTERING (AGGLOMERATIVE) ===\n",
      "2025-08-28 16:02:39 | INFO | k final: 4 | linkage: ward | metric: euclidean\n",
      "2025-08-28 16:02:39 | INFO | Silhouette final       : 0.3024\n",
      "2025-08-28 16:02:39 | INFO | Davies-Bouldin final  : 1.1118\n",
      "2025-08-28 16:02:39 | INFO | Calinski-Harabasz final: 1892.06\n",
      "2025-08-28 16:02:39 | INFO | Tamaños de cluster     : {0: 1449, 1: 1159, 2: 3110, 3: 220} (min=220)\n",
      "2025-08-28 16:02:39 | INFO | Guardado dataset con clusters (agglomerative): C:\\Users\\crisr\\Desktop\\Máster Data Science & IA\\PROYECTO\\PFM2_Asistente_Compras_Inteligente\\data\\processed\\productos_clusters_agglom.csv (filas=5938)\n",
      "2025-08-28 16:02:39 | INFO | Proceso finalizado. k_final=4 | silhouette=0.30238790374296626 | dbi=1.111834682281808 | ch=1892.0637888284389\n",
      "2025-08-28 16:02:39 | INFO | Rutas: {'clusters': 'C:\\\\Users\\\\crisr\\\\Desktop\\\\Máster Data Science & IA\\\\PROYECTO\\\\PFM2_Asistente_Compras_Inteligente\\\\data\\\\processed\\\\productos_clusters_agglom.csv', 'silhouette_vs_k': 'C:\\\\Users\\\\crisr\\\\Desktop\\\\Máster Data Science & IA\\\\PROYECTO\\\\PFM2_Asistente_Compras_Inteligente\\\\reports\\\\silhouette_vs_k_agglom.csv', 'davies_bouldin_vs_k': 'C:\\\\Users\\\\crisr\\\\Desktop\\\\Máster Data Science & IA\\\\PROYECTO\\\\PFM2_Asistente_Compras_Inteligente\\\\reports\\\\davies_bouldin_vs_k_agglom.csv', 'calinski_harabasz_vs_k': 'C:\\\\Users\\\\crisr\\\\Desktop\\\\Máster Data Science & IA\\\\PROYECTO\\\\PFM2_Asistente_Compras_Inteligente\\\\reports\\\\calinski_harabasz_vs_k_agglom.csv'}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Script: clustering_productos_agglomerative.py\n",
    "# ============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import sys, argparse, logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "# --------------------------- Raíz del proyecto ------------------------------\n",
    "def _detect_root_when_no_file():\n",
    "    here = Path().resolve()\n",
    "    for p in [here, *here.parents]:\n",
    "        if (p / \"data\").is_dir():\n",
    "            return p\n",
    "    return here\n",
    "\n",
    "if \"__file__\" in globals():\n",
    "    ROOT_DIR = Path(__file__).resolve().parents[2]\n",
    "else:\n",
    "    ROOT_DIR = _detect_root_when_no_file()\n",
    "\n",
    "DATA_DIR      = ROOT_DIR / \"data\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "REPORTS_DIR   = ROOT_DIR / \"reports\"\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --------------------------------- Logging ----------------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "logger = logging.getLogger(\"clustering_agglomerative\")\n",
    "\n",
    "# --- Parche Jupyter: elimina --f=... del kernel para argparse ---------------\n",
    "if \"ipykernel\" in sys.modules or \"IPython\" in sys.modules:\n",
    "    sys.argv = [sys.argv[0]]\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "NUM_COLS = [\n",
    "    \"d_total\", \"d_media\", \"d_std\", \"cv\", \"p95\", \"mediana\",\n",
    "    \"precio_medio\", \"PC1\", \"PC2\", \"PC3\"\n",
    "]\n",
    "\n",
    "def _check_columns(df: pd.DataFrame, cols: list[str]):\n",
    "    faltan = [c for c in cols if c not in df.columns]\n",
    "    if faltan:\n",
    "        raise KeyError(f\"Faltan columnas en el dataset de entrada: {faltan}\")\n",
    "\n",
    "def _fit_predict_agglomerative(X: np.ndarray, k: int, linkage: str, metric: str):\n",
    "    # Nota: linkage='ward' requiere metric='euclidean' en sklearn.\n",
    "    if linkage == \"ward\" and metric != \"euclidean\":\n",
    "        metric = \"euclidean\"\n",
    "    model = AgglomerativeClustering(\n",
    "        n_clusters=k, linkage=linkage, metric=metric\n",
    "    )\n",
    "    labels = model.fit_predict(X)\n",
    "    return labels\n",
    "\n",
    "def explorar_y_clusterizar(in_path: Path,\n",
    "                           out_path: Path,\n",
    "                           k_min: int = 3,\n",
    "                           k_max: int = 10,\n",
    "                           force_k: int | None = None,\n",
    "                           linkage: str = \"ward\",\n",
    "                           metric: str = \"euclidean\",\n",
    "                           sil_sample: int = 7000,\n",
    "                           random_state: int = 42):\n",
    "    # 1) Cargar\n",
    "    in_path  = Path(in_path)\n",
    "    out_path = Path(out_path)\n",
    "    logger.info(f\"Cargando features normalizadas: {in_path}\")\n",
    "    df = pd.read_csv(in_path)\n",
    "\n",
    "    if \"Product_ID\" not in df.columns:\n",
    "        raise KeyError(\"Se requiere columna 'Product_ID' en el dataset de entrada.\")\n",
    "    _check_columns(df, NUM_COLS)\n",
    "\n",
    "    X = df[NUM_COLS].astype(float).values\n",
    "    n = X.shape[0]\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    # 2) Explorar k\n",
    "    ks = list(range(max(2, k_min), max(k_min, k_max) + 1))\n",
    "    res_sil, res_dbi, res_ch = [], [], []\n",
    "\n",
    "    for k in ks:\n",
    "        logger.info(f\"[Exploración] Agglomerative (linkage={linkage}, metric={metric}) con k={k}\")\n",
    "        labels = _fit_predict_agglomerative(X, k, linkage, metric)\n",
    "\n",
    "        # Silhouette (muestreo si N muy grande)\n",
    "        if n > sil_sample:\n",
    "            idx = rng.choice(n, size=sil_sample, replace=False)\n",
    "            X_s = X[idx]; y_s = labels[idx]\n",
    "        else:\n",
    "            X_s, y_s = X, labels\n",
    "\n",
    "        try:\n",
    "            sil = float(silhouette_score(X_s, y_s, metric=\"euclidean\"))\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Silhouette falló para k={k}: {e}\")\n",
    "            sil = np.nan\n",
    "\n",
    "        # Davies-Bouldin (↓ mejor) y Calinski-Harabasz (↑ mejor) con todo X (si posible)\n",
    "        try:\n",
    "            dbi = float(davies_bouldin_score(X, labels))\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Davies-Bouldin falló para k={k}: {e}\")\n",
    "            dbi = np.nan\n",
    "        try:\n",
    "            ch = float(calinski_harabasz_score(X, labels))\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Calinski-Harabasz falló para k={k}: {e}\")\n",
    "            ch = np.nan\n",
    "\n",
    "        res_sil.append({\"k\": k, \"silhouette\": sil})\n",
    "        res_dbi.append({\"k\": k, \"davies_bouldin\": dbi})\n",
    "        res_ch.append({\"k\": k, \"calinski_harabasz\": ch})\n",
    "\n",
    "    df_sil = pd.DataFrame(res_sil)\n",
    "    df_dbi = pd.DataFrame(res_dbi)\n",
    "    df_ch  = pd.DataFrame(res_ch)\n",
    "\n",
    "    path_sil = REPORTS_DIR / \"silhouette_vs_k_agglom.csv\"\n",
    "    path_dbi = REPORTS_DIR / \"davies_bouldin_vs_k_agglom.csv\"\n",
    "    path_ch  = REPORTS_DIR / \"calinski_harabasz_vs_k_agglom.csv\"\n",
    "    df_sil.to_csv(path_sil, index=False)\n",
    "    df_dbi.to_csv(path_dbi, index=False)\n",
    "    df_ch.to_csv(path_ch, index=False)\n",
    "    logger.info(f\"Guardado: {path_sil}\")\n",
    "    logger.info(f\"Guardado: {path_dbi}\")\n",
    "    logger.info(f\"Guardado: {path_ch}\")\n",
    "\n",
    "    # 3) Selección de k\n",
    "    if force_k is not None:\n",
    "        best_k = int(force_k)\n",
    "        logger.info(f\"Usando k forzado por CLI: k={best_k}\")\n",
    "    else:\n",
    "        df_sil_valid = df_sil.dropna(subset=[\"silhouette\"])\n",
    "        if df_sil_valid.empty:\n",
    "            best_k = int(np.median(ks))\n",
    "            logger.warning(f\"No hay silhouette válido; usando k={best_k} (mediana del rango).\")\n",
    "        else:\n",
    "            max_sil = df_sil_valid[\"silhouette\"].max()\n",
    "            candidatos = df_sil_valid.loc[df_sil_valid[\"silhouette\"] == max_sil, \"k\"].tolist()\n",
    "            best_k = min(candidatos)\n",
    "            logger.info(f\"Selección automática por silhouette: k={best_k} (silhouette={max_sil:.4f})\")\n",
    "\n",
    "    # 4) Modelo final con best_k\n",
    "    labels_final = _fit_predict_agglomerative(X, best_k, linkage, metric)\n",
    "\n",
    "    # Métricas finales\n",
    "    try:\n",
    "        sil_final = float(silhouette_score(X if n <= sil_sample else X[rng.choice(n, sil_sample, replace=False)],\n",
    "                                           labels_final if n <= sil_sample else labels_final[rng.choice(n, sil_sample, replace=False)],\n",
    "                                           metric=\"euclidean\"))\n",
    "    except Exception:\n",
    "        sil_final = np.nan\n",
    "    try:\n",
    "        dbi_final = float(davies_bouldin_score(X, labels_final))\n",
    "    except Exception:\n",
    "        dbi_final = np.nan\n",
    "    try:\n",
    "        ch_final = float(calinski_harabasz_score(X, labels_final))\n",
    "    except Exception:\n",
    "        ch_final = np.nan\n",
    "\n",
    "    # Distribución tamaños\n",
    "    _, counts = np.unique(labels_final, return_counts=True)\n",
    "    dist_sizes = {int(i): int(c) for i, c in enumerate(counts)}\n",
    "    logger.info(\"=== VALIDACIÓN CLUSTERING (AGGLOMERATIVE) ===\")\n",
    "    logger.info(f\"k final: {best_k} | linkage: {linkage} | metric: {metric}\")\n",
    "    logger.info(f\"Silhouette final       : {sil_final:.4f}\" if not np.isnan(sil_final) else \"Silhouette final: NaN\")\n",
    "    logger.info(f\"Davies-Bouldin final  : {dbi_final:.4f}\" if not np.isnan(dbi_final) else \"Davies-Bouldin final: NaN\")\n",
    "    logger.info(f\"Calinski-Harabasz final: {ch_final:.2f}\" if not np.isnan(ch_final) else \"Calinski-Harabasz final: NaN\")\n",
    "    logger.info(f\"Tamaños de cluster     : {dist_sizes} (min={counts.min()})\")\n",
    "\n",
    "    # 5) Export asignaciones\n",
    "    df_out = df.copy()\n",
    "    df_out[\"Cluster_Agglo\"] = labels_final\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_out.to_csv(out_path, index=False)\n",
    "    logger.info(f\"Guardado dataset con clusters (agglomerative): {out_path} (filas={len(df_out)})\")\n",
    "\n",
    "    return {\n",
    "        \"k_final\": best_k,\n",
    "        \"linkage\": linkage,\n",
    "        \"metric\": metric,\n",
    "        \"silhouette_final\": sil_final,\n",
    "        \"davies_bouldin_final\": dbi_final,\n",
    "        \"calinski_harabasz_final\": ch_final,\n",
    "        \"sizes\": dist_sizes,\n",
    "        \"paths\": {\n",
    "            \"clusters\": str(out_path),\n",
    "            \"silhouette_vs_k\": str(path_sil),\n",
    "            \"davies_bouldin_vs_k\": str(path_dbi),\n",
    "            \"calinski_harabasz_vs_k\": str(path_ch),\n",
    "        },\n",
    "    }\n",
    "\n",
    "# ------------------------------------ CLI -----------------------------------\n",
    "def parse_args(argv=None):\n",
    "    p = argparse.ArgumentParser(description=\"Clustering aglomerativo con exploración de k y validación interna.\")\n",
    "    p.add_argument(\"--in\",       dest=\"inp\",   type=str, default=str(PROCESSED_DIR / \"productos_features_norm.csv\"))\n",
    "    p.add_argument(\"--out\",      dest=\"outp\",  type=str, default=str(PROCESSED_DIR / \"productos_clusters_agglom.csv\"))\n",
    "    p.add_argument(\"--k-min\",    dest=\"kmin\",  type=int, default=3)\n",
    "    p.add_argument(\"--k-max\",    dest=\"kmax\",  type=int, default=10)\n",
    "    p.add_argument(\"--force-k\",  dest=\"kforce\", type=int, default=None)\n",
    "    p.add_argument(\"--linkage\",  dest=\"linkage\", type=str, default=\"ward\", choices=[\"ward\",\"average\",\"complete\",\"single\"])\n",
    "    p.add_argument(\"--metric\",   dest=\"metric\",  type=str, default=\"euclidean\",\n",
    "                   help=\"Distancia para enlaces != ward. Con ward se forzará 'euclidean'.\")\n",
    "    p.add_argument(\"--sil-sample\", dest=\"silsample\", type=int, default=7000)\n",
    "    p.add_argument(\"--seed\",     dest=\"seed\",   type=int, default=42)\n",
    "\n",
    "    if argv is None and (\"ipykernel\" in sys.modules or \"IPython\" in sys.modules):\n",
    "        argv = []\n",
    "\n",
    "    args, _ = p.parse_known_args(argv)\n",
    "    logger.info(\"ARGS -> in=%s | out=%s | k=[%d..%d] | force_k=%s | linkage=%s | metric=%s | sil_sample=%d\",\n",
    "                args.inp, args.outp, args.kmin, args.kmax, str(args.kforce), args.linkage, args.metric, args.silsample)\n",
    "    return args\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    try:\n",
    "        info = explorar_y_clusterizar(\n",
    "            in_path=Path(args.inp),\n",
    "            out_path=Path(args.outp),\n",
    "            k_min=args.kmin,\n",
    "            k_max=args.kmax,\n",
    "            force_k=args.kforce,\n",
    "            linkage=args.linkage,\n",
    "            metric=args.metric,\n",
    "            sil_sample=args.silsample,\n",
    "            random_state=args.seed,\n",
    "        )\n",
    "        logger.info(\"Proceso finalizado. k_final=%s | silhouette=%s | dbi=%s | ch=%s\",\n",
    "                    info[\"k_final\"], info[\"silhouette_final\"], info[\"davies_bouldin_final\"], info[\"calinski_harabasz_final\"])\n",
    "        logger.info(\"Rutas: %s\", info[\"paths\"])\n",
    "    except Exception as e:\n",
    "        logging.exception(f\"Error en clustering aglomerativo: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91943bfc",
   "metadata": {},
   "source": [
    "\n",
    " 📊 **Resumen de métricas**\n",
    "- **Número de clusters seleccionado (k):** 4  \n",
    "- **Silhouette final:** 0.3226  \n",
    "- **Distribución de tamaños:**  \n",
    "  - Cluster 0 → 210 productos  \n",
    "  - Cluster 1 → 1233 productos  \n",
    "  - Cluster 2 → 3394 productos  \n",
    "  - Cluster 3 → 1101 productos  \n",
    "  *(mínimo tamaño: 210 productos)*  \n",
    "- **Dataset resultante:** `productos_clusters.csv` (5.938 productos × 12 columnas)\n",
    "\n",
    "\n",
    "\n",
    " ✅ **Conclusiones**\n",
    "- El modelo jerárquico (Agglomerative con enlace *ward*) ha seleccionado **k=4**, coincidiendo con el resultado obtenido previamente mediante K-Means.  \n",
    "- El **silhouette (0.3226)** es prácticamente idéntico al de K-Means, lo que indica que **ambas técnicas ofrecen un nivel de cohesión y separación muy similar**.  \n",
    "- La distribución de productos por cluster es equilibrada, sin clusters triviales o con tamaños insignificantes, lo que confirma la **robustez de la segmentación**.  \n",
    "- Dado que los resultados son consistentes entre ambos métodos, se puede concluir que la elección de **K-Means como baseline es válida y no sesgada por la técnica**.  \n",
    "- No obstante, se recomienda continuar la bitácora con otras técnicas (GMM, DBSCAN) para confirmar que no existen estructuras alternativas con mejor separación o clusters residuales que los métodos actuales no detecten.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1437c92a",
   "metadata": {},
   "source": [
    "## 3.  **Iteración 2.** \n",
    "#### ***GMM - Gaussian Mixture Models***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a05f50",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9248505a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75f092e1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d5c59b8",
   "metadata": {},
   "source": [
    "## 4.  **Iteración 3.** \n",
    "#### ***DBSCAN***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0895858c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31284bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "237e8888",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
