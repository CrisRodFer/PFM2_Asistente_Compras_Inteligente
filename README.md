# ğŸ“Œ PFM2_Asistente_Compras_Inteligente

## ğŸ“– DescripciÃ³n

Trabajo de fin de mÃ¡ster en **Data Science & IA**.\
Este proyecto consiste en el desarrollo de un **Asistente Inteligente de
Compras**, cuyo objetivo es optimizar la gestiÃ³n de inventario y stock
de una empresa mediante tÃ©cnicas de anÃ¡lisis de datos, simulaciÃ³n de
demanda y modelado predictivo.

------------------------------------------------------------------------

## ğŸ“‚ Estructura del proyecto

``` plaintext
PFM2_Asistente_Compras_Inteligente/
â”œâ”€â”€ data/               # Datos en diferentes estados
â”‚   â”œâ”€â”€ raw/            # Datos originales sin procesar
â”‚   â”œâ”€â”€ interim/        # Datos intermedios (transformaciones temporales)
â”‚   â”œâ”€â”€ clean/          # Datos limpios y validados
â”‚   â”œâ”€â”€ processed/      # Datos preparados
â”‚   â””â”€â”€ reports/        # Reportes automÃ¡ticos (ej. huecos en histÃ³ricos)
â”œâ”€â”€ outputs/            # Resultados y salidas
â”‚   â”œâ”€â”€ figures/        # GrÃ¡ficas
â”‚   â””â”€â”€ reports/        # Informes
â”œâ”€â”€ logs/               # Registros de ejecuciÃ³n
â”œâ”€â”€ notebooks/          # Notebooks del proyecto
â”œâ”€â”€ scripts/            # Scripts ejecutables
â”œâ”€â”€ sql/                # Consultas SQL
â”œâ”€â”€ streamlit_app/      # Interfaz Streamlit
â”œâ”€â”€ src/                # CÃ³digo reusable
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ logging_conf.py
â”‚   â””â”€â”€ utils_data.py
â”œâ”€â”€ requirements.txt    # Dependencias
â”œâ”€â”€ .gitignore
â””â”€â”€ .gitattributes
```

------------------------------------------------------------------------

## ğŸš€ InstalaciÃ³n

1.  Clonar el repositorio:

    ``` bash
    git clone https://github.com/<usuario>/PFM2_Asistente_Compras_Inteligente.git
    ```

2.  Crear y activar un entorno virtual:

    ``` bash
    python -m venv venv
    source venv/bin/activate  # En Linux/Mac
    venv\Scripts\activate     # En Windows
    ```

3.  Instalar dependencias:

    ``` bash
    pip install -r requirements.txt
    ```

------------------------------------------------------------------------

## ğŸ¯ Objetivos del proyecto

-   SimulaciÃ³n de la demanda base diaria.
-   OptimizaciÃ³n de compras segÃºn stock disponible.
-   IdentificaciÃ³n de proveedores crÃ­ticos.
-   Desarrollo de una interfaz en Streamlit para interactuar con el
    modelo.
-   IntegraciÃ³n futura con SQL para gestiÃ³n eficiente de catÃ¡logos y
    consultas.

------------------------------------------------------------------------

## ğŸ“Š TecnologÃ­as utilizadas

-   **Python**
-   **Pandas / NumPy**
-   **Matplotlib / Seaborn**
-   **Scikit-learn**
-   **Streamlit**
-   **SQLite**
-   **Git / GitHub**

------------------------------------------------------------------------

ğŸ‘‰ A partir de aquÃ­, iremos completando con: 
- **Detalles de datasets** 
- **Proceso metodolÃ³gico (EDA, simulaciones, modelado,validaciÃ³n)** 
- **Resultados** 
- **LÃ­neas futuras**

------------------------------------------------------------------------

La carpeta data/ incluye un README especÃ­fico donde se documenta en detalle la 
estructura de las subcarpetas de datos. Lo mismo ocurre con la carpeta scripts/.

------------------------------------------------------------------------

## ğŸ“‘ MetodologÃ­a â€“ Fase 1 (EDA y preparaciÃ³n de datos)

Durante esta fase se llevaron a cabo las siguientes tareas:

1. **Limpieza y validaciÃ³n de la previsiÃ³n de demanda 2025**  
   - RevisiÃ³n de fechas (inclusiÃ³n de 29/02 y 31/12).  
   - EliminaciÃ³n de nulos y duplicados.  
   - GeneraciÃ³n de reportes automÃ¡ticos de huecos.  

2. **GeneraciÃ³n de histÃ³ricos simulados (2022â€“2024)**  
   - CreaciÃ³n de histÃ³ricos diarios a partir de la previsiÃ³n de 2025.  
   - Ajustes por coherencia temporal y progresiÃ³n lÃ³gica entre aÃ±os.  
   - ExportaciÃ³n en CSV/Parquet y reporte de validaciÃ³n.  

3. **Limpieza del catÃ¡logo de productos**  
   - NormalizaciÃ³n de caracteres y eliminaciÃ³n de inconsistencias.  
   - ReordenaciÃ³n de columnas y exportaciÃ³n en formato limpio.  
   - ExportaciÃ³n final en Excel y Parquet para integraciÃ³n con la demanda.  

4. **AnÃ¡lisis de coherencia entre histÃ³ricos (2022â€“2024)**  
   - **Visual**: evoluciÃ³n mensual y boxplots de medias por producto.  
   - **EstadÃ­stico**: descriptivos (media, std, CV) y correlaciones interanuales (>0.98).  
   - **Contrastes de hipÃ³tesis**: ANOVA y Kruskal-Wallis confirman ausencia de diferencias significativas entre aÃ±os.  

ğŸ“Œ **ConclusiÃ³n de la fase**:  
Los histÃ³ricos generados presentan un comportamiento **estable, coherente y robusto**, validando que los datos estÃ¡n en condiciones Ã³ptimas para pasar a la siguiente fase de desagregaciÃ³n de demanda y modelado predictivo.

------------------------------------------------------------------------

### JustificaciÃ³n del uso de patrones estacionales

Aunque el dataset inicial es artificial, se ha enriquecido mediante la aplicaciÃ³n de un **patrÃ³n estacional basado en factores reales del comercio electrÃ³nico en EspaÃ±a** (ciclos de ingresos mensuales, estacionalidad semanal, campaÃ±as clave como rebajas, Black Friday o Navidad, y ajustes en festivos/vacaciones).  

El objetivo no es replicar al detalle un histÃ³rico pasado, sino **incorporar estructuras realistas** que permitan que los modelos de predicciÃ³n entrenados posteriormente sean **robustos y aplicables a escenarios futuros**.  
De este modo, cada ajuste o modificaciÃ³n sobre el calendario se entiende como una **etapa de calibraciÃ³n deliberada**, cuyo fin es generar un dataset artificial mÃ¡s realista, coherente y predictivo.  

-------------------------------------------------------------------------

##  ğŸ“‘ MetodologÃ­a â€“ Fase 2 (GeneraciÃ³n de histÃ³ricos y validaciÃ³n estacional).

1. **PatrÃ³n estacional**
   - Calendario artificial enriquecido con factores del ecommerce en EspaÃ±a (rebajas, Black Friday, Prime Day, Navidad, Semana Santa, festivos y agosto).
   - La funciÃ³n `generar_calendario_estacional(...)` se aplica por aÃ±o sin I/O y los scripts guardan el calendario por aÃ±o.

2. **DesagregaciÃ³n de demanda anual**
   - ConversiÃ³n a demanda **diaria** para 2022, 2023 y 2024.
   - Entradas: `data/processed/demanda_diaria_YYYY.csv` (columnas: `Date`, `Demand_Day`).
   - Salidas EDA: `outputs/figures/*` y `outputs/tables/*`.

3. **Comparativa entre aÃ±os (EDA)**
   - EvoluciÃ³n diaria total por aÃ±o, medias mensuales con CV, correlaciones interanuales y KPIs de consistencia.

4. **ValidaciÃ³n con calendario real (iterativa)**
   - Objetivo: comprobar si los picos/vales de la demanda desagregada coinciden con eventos reales.
   - Se probaron diferentes enfoques: baseline mensual, filtro de activos, baseline DoW, baseline local Â±k y ventanas desplazadas Â±shift.
   - **ConfiguraciÃ³n final** seleccionada: **baseline local Â±7 con ventanas Â±3**.
   - Salidas:
     - `outputs/tables/validacion_calendario_real_SHIFT_localk7_s3_YYYY.csv`
     - `outputs/tables/validacion_calendario_real_kpis_SHIFT_k7_s3.csv`
     - `outputs/figures/evolucion_2024_con_eventos_SHIFT_k7_s3.png`

ğŸ““ **Nota metodolÃ³gica (validaciÃ³n estacional)**  
Se elaborÃ³ un **Notebook bitÃ¡cora** con todas las iteraciones de validaciÃ³n frente a calendario real.  
Este README y el notebook **PFM2_Fase2** recogen Ãºnicamente la **versiÃ³n final consolidada** (baseline local Â±7 y ventanas Â±3).  
La bitÃ¡cora estÃ¡ disponible en la carpeta `/notebooks/` como material complementario.


### Reproducibilidad (script)

# Desde la raÃ­z del proyecto
python scripts/eda/validacion_calendario_real.py \
  --years 2022 2023 2024 --k 7 --shift 3


ğŸ“Œ ConclusiÃ³n de la Fase 2:
La validaciÃ³n estacional confirma que los histÃ³ricos desagregados reflejan un comportamiento coherente con eventos de mercado. La configuraciÃ³n final 
seleccionada (baseline local Â±7 y ventanas Â±3) se utilizarÃ¡ como feature base en la siguiente fase, donde se integrarÃ¡n variables de precio y externas.

-------------------------------------------------------------------------
## ğŸ“‘ MetodologÃ­a â€“ Fase 3 (Clustering de productos y generaciÃ³n de subset representativo)

### 3.1 PreparaciÃ³n de features de producto
- Limpieza y normalizaciÃ³n de categorÃ­as.
- EliminaciÃ³n de productos sin PCs relevantes (categorÃ­as no representativas).
- ObtenciÃ³n de `productos_features_clean.csv` como base de entrada al anÃ¡lisis de clustering.

### 3.2 Clustering de productos
- ComparaciÃ³n de tÃ©cnicas: **K-Means, GMM y DBSCAN**.
- MÃ©tricas internas: silhouette score (K-Means con *k=4* â‰ˆ 0.32).
- InterpretaciÃ³n de clusters:
  - **C0** â†’ nicho reducido (~210 productos).
  - **C1** â†’ productos estables y de alta demanda (~1.233).
  - **C2** â†’ cluster mayoritario (~3.394).
  - **C3** â†’ miscelÃ¡nea (~1.101).
- DecisiÃ³n final: **K-Means (k=4)** como modelo de referencia.  
  GMM y DBSCAN se emplearon como tÃ©cnicas de contraste.

### 3.3 ValidaciÃ³n complementaria del clustering
- Uso del notebook auxiliar `PFM2_Fase3_pruebas_clustering.ipynb`.
- ConfirmaciÃ³n de la robustez de K-Means frente a alternativas.

### 3.4 AsignaciÃ³n de clusters a la demanda desagregada
- Script `asignar_cluster_a_demanda.py`:
  - **Entrada**: `demanda_filtrada_enriquecida_sin_nans.csv` + `productos_clusters.csv`.
  - **Salida**: `demanda_con_clusters.csv`.
  - ExclusiÃ³n de productos sin cluster asignado (~113 productos, <2%).
- Validaciones:
  - Cobertura temporal completa.
  - AsignaciÃ³n de clusters consistente con el catÃ¡logo.

### 3.5 GeneraciÃ³n del subset representativo
- **Criterios de selecciÃ³n**:
  - Mantener cobertura completa de fechas (2022â€“2024).
  - Conservar casi completos los clÃºsteres minoritarios (C0, C1, C3).
  - Reducir proporcionalmente el clÃºster mayoritario (C2) al ~30%.
  - Incluir **todos los outliers** como casos especiales (no eliminar top ventas atÃ­picos).
- **Outputs intermedios**:
  - `demanda_subset.parquet` (30% sin outliers).
  - `outliers.parquet` (productos con `is_outlier=1`).
  - `demanda_subset_final.parquet` (fusiÃ³n subset + outliers).

### 3.6 Validaciones del subset final
- **ValidaciÃ³n rÃ¡pida pre-outliers**:
  - Reglas de reducciÃ³n de clusters aplicadas correctamente.
  - Integridad de claves sin duplicados ni nulos.
- **ValidaciÃ³n robusta final** (sobre parquet):
  - Cobertura temporal 2022â€“2024 intacta.
  - Sin duplicados ni NaNs en claves.
  - Subset âŠ† catÃ¡logo original.
  - Todos los outliers conservados (220.296 filas, 201 productos).
  - DistribuciÃ³n por cluster coherente tras reducciÃ³n.
  - TamaÃ±o final: 3.596 productos (~60% del catÃ¡logo).


ğŸ“Œ **ConclusiÃ³n de la Fase 3**:  
Se ha construido un **subset representativo, coherente y manejable** (30% + outliers), que mantiene diversidad de clusters, top ventas atÃ­picos y cobertura temporal completa. Este subset servirÃ¡ como base para la **Fase 4**, centrada en el anÃ¡lisis del impacto del precio y variables externas.
